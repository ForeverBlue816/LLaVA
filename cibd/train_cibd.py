#!/usr/bin/env python3
"""
CIBDè®­ç»ƒè„šæœ¬ - V2 with RDO
çœŸæ­£é›†æˆäº†ç‡å¤±çœŸä¼˜åŒ–å™¨å’Œè‡ªé€‚åº”è°ƒåº¦å™¨
"""

import os
import sys
import json
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import get_cosine_schedule_with_warmup
import argparse
from tqdm import tqdm
import logging
from typing import Optional, Dict, Any
import numpy as np

# å¯¼å…¥LLaVAç»„ä»¶
from llava.model.builder import load_pretrained_model
from llava.model.language_model.llava_llama import LlavaLlamaForCausalLM
from transformers.modeling_outputs import CausalLMOutputWithPast

# å¯¼å…¥CIBDç»„ä»¶ï¼ˆåªä½¿ç”¨V2ï¼‰
from cibd.dataset import LLaVADataset, LLaVACollator
from cibd.llava_cibd_model_v2 import LlavaCIBDModelV2
from cibd.create_student_model_v2 import (
    create_compressed_student_config_v2,
    initialize_student_from_teacher_v2,
    estimate_model_params
)

# å¯¼å…¥RDOç»„ä»¶
from cibd.rate_distortion_optimizer import (
    RateDistortionOptimizer,
    AdaptiveRateScheduler
)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    format='%(asctime)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)


class CIBDTrainerWithRDO:
    """
    CIBDè®­ç»ƒå™¨ - é›†æˆRDOä¼˜åŒ–
    
    æ–°åŠŸèƒ½ï¼š
    1. åŠ¨æ€è°ƒæ•´ Î² (log_beta) å‚æ•°
    2. è‡ªé€‚åº”å‹ç¼©ç‡è°ƒåº¦
    3. Paretoå‰æ²¿è¿½è¸ª
    """
    
    def __init__(self, model, train_loader, val_loader, args, device):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.args = args
        self.device = device
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=args.learning_rate,
            weight_decay=args.weight_decay
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        num_training_steps = len(train_loader) * args.num_epochs
        num_warmup_steps = int(num_training_steps * args.warmup_ratio)
        
        self.scheduler = get_cosine_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=num_warmup_steps,
            num_training_steps=num_training_steps
        )
        
        # ===== æ–°å¢ï¼šRDO ä¼˜åŒ–å™¨ =====
        self.rdo = RateDistortionOptimizer(
            initial_beta=args.initial_beta,
            target_compression=args.target_compression_rate,
            adaptation_rate=args.beta_adaptation_rate,
            window_size=100
        )
        
        # ===== æ–°å¢ï¼šè‡ªé€‚åº”å‹ç¼©ç‡è°ƒåº¦å™¨ =====
        if args.use_adaptive_compression:
            self.compression_scheduler = AdaptiveRateScheduler(
                initial_rate=args.initial_compression_rate,
                target_rate=args.target_compression_rate,
                warmup_steps=num_warmup_steps,
                total_steps=num_training_steps
            )
        else:
            self.compression_scheduler = None
        
        # è®­ç»ƒçŠ¶æ€
        self.global_step = 0
        self.current_epoch = 0
        self.best_val_loss = float('inf')
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(args.output_dir, exist_ok=True)
        self.checkpoint_dir = os.path.join(args.output_dir, 'checkpoints')
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
        # æ—¥å¿—æ–‡ä»¶
        self.log_file = os.path.join(args.output_dir, 'training_log.txt')
        self.rdo_log_file = os.path.join(args.output_dir, 'rdo_log.txt')
        
        logger.info("\n" + "=" * 60)
        logger.info("è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆï¼ˆwith RDOï¼‰")
        logger.info("=" * 60)
        logger.info(f"è®­ç»ƒæ ·æœ¬: {len(train_loader.dataset)}")
        logger.info(f"éªŒè¯æ ·æœ¬: {len(val_loader.dataset) if val_loader else 0}")
        logger.info(f"æ€»æ­¥æ•°: {num_training_steps}")
        logger.info(f"é¢„çƒ­æ­¥æ•°: {num_warmup_steps}")
        logger.info(f"å­¦ä¹ ç‡: {args.learning_rate}")
        logger.info(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
        logger.info(f"æ¢¯åº¦ç´¯ç§¯: {args.gradient_accumulation_steps}")
        logger.info(f"\nğŸ”§ RDO é…ç½®:")
        logger.info(f"  åˆå§‹ Î²: {args.initial_beta}")
        logger.info(f"  ç›®æ ‡å‹ç¼©ç‡: {args.target_compression_rate}")
        logger.info(f"  Î² è‡ªé€‚åº”ç‡: {args.beta_adaptation_rate}")
        logger.info(f"  è‡ªé€‚åº”å‹ç¼©: {args.use_adaptive_compression}")
        logger.info("=" * 60 + "\n")
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        logger.info("\n" + "=" * 60)
        logger.info("å¼€å§‹è®­ç»ƒï¼ˆwith RDOï¼‰")
        logger.info("=" * 60 + "\n")
        
        for epoch in range(self.current_epoch, self.args.num_epochs):
            self.current_epoch = epoch
            
            logger.info(f"\n{'='*60}")
            logger.info(f"Epoch {epoch + 1}/{self.args.num_epochs}")
            logger.info(f"{'='*60}")
            
            # è®­ç»ƒä¸€ä¸ªepoch
            train_metrics = self._train_epoch()
            
            # è®°å½•è®­ç»ƒæŒ‡æ ‡
            self._log_metrics(epoch, train_metrics, phase='train')
            
            # éªŒè¯
            if self.val_loader is not None:
                val_metrics = self._validate()
                self._log_metrics(epoch, val_metrics, phase='val')
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_metrics['total_loss'] < self.best_val_loss:
                    self.best_val_loss = val_metrics['total_loss']
                    self._save_checkpoint('best_model')
                    logger.info(f"âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (val_loss={self.best_val_loss:.4f})")
            
            # å®šæœŸä¿å­˜checkpoint
            if (epoch + 1) % self.args.save_steps == 0:
                self._save_checkpoint(f'checkpoint_epoch_{epoch+1}')
            
            # ä¿å­˜Paretoå‰æ²¿
            self._save_pareto_frontier(epoch)
        
        logger.info("\n" + "=" * 60)
        logger.info("è®­ç»ƒå®Œæˆ!")
        logger.info("=" * 60 + "\n")
    
    def _train_epoch(self):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        total_loss = 0
        loss_components = {
            'task_loss': 0,
            'kl_loss': 0,
            'feature_loss': 0,
            'ib_loss': 0,
            'layer_0_loss': 0,
            'layer_1_loss': 0,
            'layer_2_loss': 0,
        }
        
        # RDO ç»Ÿè®¡
        rdo_stats = {
            'beta_values': [],
            'compression_rates': [],
            'rate_values': [],
            'distortion_values': []
        }
        
        self.optimizer.zero_grad()
        
        progress_bar = tqdm(
            self.train_loader,
            desc=f"Epoch {self.current_epoch + 1}",
            dynamic_ncols=True
        )
        
        for step, batch in enumerate(progress_bar):
            # ç§»åŠ¨åˆ°è®¾å¤‡
            batch = {k: v.to(self.device) if torch.is_tensor(v) else v 
                    for k, v in batch.items()}
            
            # ===== 1. è‡ªé€‚åº”å‹ç¼©ç‡è°ƒåº¦ =====
            if self.compression_scheduler is not None:
                current_compression_rate = self.compression_scheduler.get_compression_rate()
                # å¯ä»¥ç”¨è¿™ä¸ªè°ƒæ•´ IB çš„ç“¶é¢ˆç»´åº¦æˆ–å…¶ä»–å‚æ•°
                # è¿™é‡Œæˆ‘ä»¬å°†å®ƒè®°å½•ä¸‹æ¥
                rdo_stats['compression_rates'].append(current_compression_rate)
            
            # å‰å‘ä¼ æ’­
            outputs = self.model(**batch)
            loss = outputs.loss
            
            # ===== 2. RDO ä¼˜åŒ–ï¼šåŠ¨æ€è°ƒæ•´ Î² =====
            if hasattr(outputs, 'loss_info'):
                task_loss = outputs.loss_info.get('task_loss', 0)
                ib_loss = outputs.loss_info.get('ib_loss', 0)
                
                # å¦‚æœæœ‰ IB æŸå¤±ï¼Œä½¿ç”¨ RDO ä¼˜åŒ–
                if ib_loss > 0 and task_loss > 0:
                    # è®¡ç®—ç‡ï¼ˆIB KLï¼‰å’Œå¤±çœŸï¼ˆä»»åŠ¡æŸå¤±ï¼‰
                    rate = outputs.loss_info.get('ib_kl', ib_loss)
                    distortion = task_loss
                    
                    # RDO æ­¥éª¤ï¼šè®¡ç®—æœ€ä¼˜ Î²
                    optimal_beta = self.rdo.step(
                        torch.tensor(distortion).to(self.device),
                        torch.tensor(rate).to(self.device)
                    )
                    
                    # ===== å…³é”®ï¼šæ›´æ–°æ¨¡å‹çš„ log_beta =====
                    if hasattr(self.model, 'visual_ib') and hasattr(self.model.visual_ib, 'log_beta'):
                        with torch.no_grad():
                            self.model.visual_ib.log_beta.data = torch.tensor(
                                np.log(optimal_beta),
                                dtype=torch.float32,
                                device=self.device
                            )
                    
                    # è®°å½• RDO ç»Ÿè®¡
                    rdo_stats['beta_values'].append(optimal_beta)
                    rdo_stats['rate_values'].append(rate)
                    rdo_stats['distortion_values'].append(distortion)
            
            # æ¢¯åº¦ç´¯ç§¯
            loss = loss / self.args.gradient_accumulation_steps
            loss.backward()
            
            # ç´¯ç§¯æŸå¤±
            total_loss += loss.item() * self.args.gradient_accumulation_steps
            
            # ç´¯ç§¯æŸå¤±ç»„ä»¶
            if hasattr(outputs, 'loss_info'):
                for key in loss_components:
                    if key in outputs.loss_info:
                        loss_components[key] += outputs.loss_info[key]
            
            # æ›´æ–°å‚æ•°
            if (step + 1) % self.args.gradient_accumulation_steps == 0:
                # æ¢¯åº¦è£å‰ª
                if self.args.max_grad_norm > 0:
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        self.args.max_grad_norm
                    )
                
                self.optimizer.step()
                self.scheduler.step()
                self.optimizer.zero_grad()
                self.global_step += 1
                
                # æ›´æ–°è¿›åº¦æ¡
                current_lr = self.scheduler.get_last_lr()[0]
                current_beta = rdo_stats['beta_values'][-1] if rdo_stats['beta_values'] else 1.0
                progress_bar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'lr': f'{current_lr:.2e}',
                    'Î²': f'{current_beta:.3f}'
                })
        
        # è®¡ç®—å¹³å‡
        num_batches = len(self.train_loader)
        metrics = {
            'total_loss': total_loss / num_batches,
            **{k: v / num_batches for k, v in loss_components.items()}
        }
        
        # æ·»åŠ  RDO ç»Ÿè®¡
        if rdo_stats['beta_values']:
            metrics['avg_beta'] = np.mean(rdo_stats['beta_values'])
            metrics['std_beta'] = np.std(rdo_stats['beta_values'])
            metrics['avg_rate'] = np.mean(rdo_stats['rate_values'])
            metrics['avg_distortion'] = np.mean(rdo_stats['distortion_values'])
        
        if rdo_stats['compression_rates']:
            metrics['avg_compression_rate'] = np.mean(rdo_stats['compression_rates'])
        
        # è®°å½• RDO æ—¥å¿—
        self._log_rdo_stats(rdo_stats)
        
        return metrics
    
    def _validate(self):
        """éªŒè¯"""
        self.model.eval()
        
        total_loss = 0
        loss_components = {
            'task_loss': 0,
            'kl_loss': 0,
            'feature_loss': 0,
            'ib_loss': 0,
            'layer_0_loss': 0,
            'layer_1_loss': 0,
            'layer_2_loss': 0,
        }
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc="Validation", dynamic_ncols=True):
                # ç§»åŠ¨åˆ°è®¾å¤‡
                batch = {k: v.to(self.device) if torch.is_tensor(v) else v 
                        for k, v in batch.items()}
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(**batch)
                loss = outputs.loss
                
                total_loss += loss.item()
                
                # ç´¯ç§¯æŸå¤±ç»„ä»¶
                if hasattr(outputs, 'loss_info'):
                    for key in loss_components:
                        if key in outputs.loss_info:
                            loss_components[key] += outputs.loss_info[key]
        
        # è®¡ç®—å¹³å‡
        num_batches = len(self.val_loader)
        metrics = {
            'total_loss': total_loss / num_batches,
            **{k: v / num_batches for k, v in loss_components.items()}
        }
        
        return metrics
    
    def _log_metrics(self, epoch, metrics, phase='train'):
        """è®°å½•æŒ‡æ ‡"""
        log_str = f"\n[{phase.upper()}] Epoch {epoch + 1}\n"
        log_str += f"  Total Loss: {metrics['total_loss']:.4f}\n"
        log_str += f"  Task Loss: {metrics['task_loss']:.4f}\n"
        log_str += f"  KL Loss: {metrics['kl_loss']:.4f}\n"
        log_str += f"  Feature Loss: {metrics['feature_loss']:.4f}\n"
        log_str += f"  IB Loss: {metrics['ib_loss']:.4f}\n"
        
        # RDO æŒ‡æ ‡
        if 'avg_beta' in metrics:
            log_str += f"\n  ğŸ”§ RDO Stats:\n"
            log_str += f"    Avg Î²: {metrics['avg_beta']:.4f} Â± {metrics['std_beta']:.4f}\n"
            log_str += f"    Avg Rate: {metrics['avg_rate']:.4f}\n"
            log_str += f"    Avg Distortion: {metrics['avg_distortion']:.4f}\n"
        
        if 'avg_compression_rate' in metrics:
            log_str += f"    Compression Rate: {metrics['avg_compression_rate']:.4f}\n"
        
        logger.info(log_str)
        
        # å†™å…¥æ–‡ä»¶
        with open(self.log_file, 'a') as f:
            f.write(log_str + "\n")
    
    def _log_rdo_stats(self, rdo_stats):
        """è®°å½• RDO è¯¦ç»†ç»Ÿè®¡"""
        if not rdo_stats['beta_values']:
            return
        
        log_str = f"\nStep {self.global_step} RDO Stats:\n"
        log_str += f"  Î²: {np.mean(rdo_stats['beta_values']):.4f}\n"
        log_str += f"  Rate: {np.mean(rdo_stats['rate_values']):.4f}\n"
        log_str += f"  Distortion: {np.mean(rdo_stats['distortion_values']):.4f}\n"
        
        with open(self.rdo_log_file, 'a') as f:
            f.write(log_str)
    
    def _save_pareto_frontier(self, epoch):
        """ä¿å­˜Paretoå‰æ²¿"""
        rates, distortions = self.rdo.get_pareto_frontier()
        
        if rates:
            pareto_data = {
                'epoch': epoch,
                'rates': rates,
                'distortions': distortions
            }
            
            pareto_file = os.path.join(
                self.args.output_dir, 
                f'pareto_frontier_epoch_{epoch}.json'
            )
            
            with open(pareto_file, 'w') as f:
                json.dump(pareto_data, f, indent=2)
            
            logger.info(f"âœ“ Saved Pareto frontier: {len(rates)} points")
    
    def _save_checkpoint(self, name):
        """ä¿å­˜checkpoint"""
        checkpoint_path = os.path.join(self.checkpoint_dir, name)
        os.makedirs(checkpoint_path, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹
        self.model.save_pretrained(checkpoint_path)
        
        # ä¿å­˜è®­ç»ƒçŠ¶æ€
        state = {
            'epoch': self.current_epoch,
            'global_step': self.global_step,
            'best_val_loss': self.best_val_loss,
            'optimizer': self.optimizer.state_dict(),
            'scheduler': self.scheduler.state_dict(),
            'rdo_beta': self.rdo.beta,
            'rdo_history': {
                'rate_history': list(self.rdo.rate_history),
                'distortion_history': list(self.rdo.distortion_history),
                'beta_history': list(self.rdo.beta_history),
            }
        }
        
        if self.compression_scheduler is not None:
            state['compression_scheduler_step'] = self.compression_scheduler.current_step
        
        torch.save(state, os.path.join(checkpoint_path, 'trainer_state.pt'))
        
        logger.info(f"âœ“ Checkpoint saved: {checkpoint_path}")
    
    def load_checkpoint(self, checkpoint_path):
        """åŠ è½½checkpoint"""
        logger.info(f"åŠ è½½checkpoint: {checkpoint_path}")
        
        # åŠ è½½è®­ç»ƒçŠ¶æ€
        state_path = os.path.join(checkpoint_path, 'trainer_state.pt')
        if os.path.exists(state_path):
            state = torch.load(state_path, map_location=self.device)
            self.current_epoch = state['epoch']
            self.global_step = state['global_step']
            self.best_val_loss = state['best_val_loss']
            self.optimizer.load_state_dict(state['optimizer'])
            self.scheduler.load_state_dict(state['scheduler'])
            
            # æ¢å¤ RDO çŠ¶æ€
            if 'rdo_beta' in state:
                self.rdo.beta = state['rdo_beta']
            
            if 'rdo_history' in state:
                from collections import deque
                self.rdo.rate_history = deque(state['rdo_history']['rate_history'], maxlen=100)
                self.rdo.distortion_history = deque(state['rdo_history']['distortion_history'], maxlen=100)
                self.rdo.beta_history = deque(state['rdo_history']['beta_history'], maxlen=100)
            
            if 'compression_scheduler_step' in state and self.compression_scheduler is not None:
                self.compression_scheduler.current_step = state['compression_scheduler_step']
            
            logger.info(f"âœ“ ä»epoch {self.current_epoch} æ¢å¤è®­ç»ƒ")
            logger.info(f"âœ“ RDO Î² æ¢å¤ä¸º: {self.rdo.beta:.4f}")


def parse_args():
    parser = argparse.ArgumentParser(description="CIBD Training Script V2 with RDO")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--teacher_model_path", type=str, required=True,
                       help="æ•™å¸ˆæ¨¡å‹è·¯å¾„")
    parser.add_argument("--compression_ratio", type=float, default=0.5,
                       help="å‹ç¼©æ¯”ç‡ (0.3-0.7)")
    parser.add_argument("--compress_hidden", action="store_true",
                       help="æ˜¯å¦å‹ç¼©hidden_size")
    parser.add_argument("--compress_heads", action="store_true",
                       help="æ˜¯å¦å‹ç¼©attention heads")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--train_data_path", type=str, required=True,
                       help="è®­ç»ƒæ•°æ®JSONè·¯å¾„")
    parser.add_argument("--val_data_path", type=str, default=None,
                       help="éªŒè¯æ•°æ®JSONè·¯å¾„")
    parser.add_argument("--image_folder", type=str, required=True,
                       help="å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„")
    parser.add_argument("--auto_split", action="store_true",
                       help="è‡ªåŠ¨åˆ†å‰²è®­ç»ƒ/éªŒè¯é›†")
    parser.add_argument("--val_split", type=float, default=0.1,
                       help="éªŒè¯é›†æ¯”ä¾‹ï¼ˆç”¨äºauto_splitï¼‰")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--batch_size", type=int, default=4)
    parser.add_argument("--num_epochs", type=int, default=3)
    parser.add_argument("--learning_rate", type=float, default=2e-5)
    parser.add_argument("--weight_decay", type=float, default=0.01)
    parser.add_argument("--warmup_ratio", type=float, default=0.03)
    parser.add_argument("--max_grad_norm", type=float, default=1.0)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1)
    
    # ===== RDO å‚æ•° =====
    parser.add_argument("--initial_beta", type=float, default=1.0,
                       help="åˆå§‹ Î² å€¼")
    parser.add_argument("--target_compression_rate", type=float, default=0.5,
                       help="ç›®æ ‡å‹ç¼©ç‡ï¼ˆç”¨äºRDOï¼‰")
    parser.add_argument("--beta_adaptation_rate", type=float, default=0.01,
                       help="Î² è‡ªé€‚åº”å­¦ä¹ ç‡")
    parser.add_argument("--use_adaptive_compression", action="store_true",
                       help="ä½¿ç”¨è‡ªé€‚åº”å‹ç¼©ç‡è°ƒåº¦")
    parser.add_argument("--initial_compression_rate", type=float, default=0.1,
                       help="åˆå§‹å‹ç¼©ç‡ï¼ˆç”¨äºè°ƒåº¦å™¨ï¼‰")
    
    # æ•°æ®åŠ è½½
    parser.add_argument("--num_workers", type=int, default=4)
    parser.add_argument("--max_length", type=int, default=2048)
    
    # ä¿å­˜å’Œæ—¥å¿—
    parser.add_argument("--output_dir", type=str, required=True)
    parser.add_argument("--save_steps", type=int, default=1,
                       help="æ¯Nä¸ªepochä¿å­˜ä¸€æ¬¡")
    parser.add_argument("--resume", type=str, default=None,
                       help="æ¢å¤è®­ç»ƒçš„checkpointè·¯å¾„")
    
    return parser.parse_args()


def main():
    args = parse_args()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    logger.info("\n" + "=" * 60)
    logger.info("CIBD Training V2 with RDO")
    logger.info("=" * 60)
    logger.info(f"Device: {device}")
    logger.info(f"Compression ratio: {args.compression_ratio}")
    logger.info(f"Output dir: {args.output_dir}")
    logger.info(f"RDO enabled: âœ“")
    logger.info(f"Initial Î²: {args.initial_beta}")
    logger.info(f"Target compression rate: {args.target_compression_rate}")
    logger.info("=" * 60 + "\n")
    
    # 1. åŠ è½½æ•™å¸ˆæ¨¡å‹
    logger.info("\n" + "=" * 60)
    logger.info("åŠ è½½æ•™å¸ˆæ¨¡å‹...")
    logger.info("=" * 60)
    
    tokenizer, teacher_model, image_processor, context_len = load_pretrained_model(
        model_path=args.teacher_model_path,
        model_base=None,
        model_name=args.teacher_model_path.split('/')[-1],
        device_map=None
    )
    
    # ç¡®ä¿vocabå¤§å°æ­£ç¡®
    logger.info(f"Tokenizer vocab size: {len(tokenizer)}")
    if hasattr(teacher_model.config, 'vocab_size'):
        tokenizer_vocab_size = len(tokenizer)
        if teacher_model.config.vocab_size != tokenizer_vocab_size:
            logger.warning(f"  âš ï¸  vocab_sizeä¸åŒ¹é…:")
            logger.info(f"      config: {teacher_model.config.vocab_size}")
            logger.info(f"      tokenizer: {tokenizer_vocab_size}")
            teacher_model.config.vocab_size = tokenizer_vocab_size
            logger.info(f"  âœ“ å·²æ›´æ–°ä¸º: {tokenizer_vocab_size}")

    teacher_model.to(device)
    teacher_model.eval()
    
    teacher_params = sum(p.numel() for p in teacher_model.parameters())
    logger.info(f"âœ“ æ•™å¸ˆæ¨¡å‹: {teacher_params/1e6:.2f}M å‚æ•°")
    
    # 2. åˆ›å»ºå­¦ç”Ÿæ¨¡å‹
    logger.info("\n" + "=" * 60)
    logger.info("åˆ›å»ºå­¦ç”Ÿæ¨¡å‹...")
    logger.info("=" * 60)
    
    teacher_config = teacher_model.config
    student_config = create_compressed_student_config_v2(
        teacher_config,
        compression_ratio=args.compression_ratio,
        compress_hidden=args.compress_hidden,
        compress_heads=args.compress_heads
    )
    
    logger.info(f"\nå‹ç¼©ç­–ç•¥:")
    logger.info(f"  compress_hidden: {args.compress_hidden}")
    logger.info(f"  compress_heads: {args.compress_heads}")
    
    # ä½¿ç”¨ V2 æ¨¡å‹
    student_model = LlavaCIBDModelV2(student_config, teacher_model)
    
    # æ£€æŸ¥å¹¶éªŒè¯ embedding å¤§å°
    logger.info("\næ£€æŸ¥å­¦ç”Ÿæ¨¡å‹ embedding å±‚...")
    if hasattr(student_model.model, 'embed_tokens'):
        actual_embed_vocab = student_model.model.embed_tokens.num_embeddings
        logger.info(f"  embed_tokens.num_embeddings: {actual_embed_vocab}")
        
        if actual_embed_vocab != len(tokenizer):
            logger.warning(f"  âš ï¸  Embedding å¤§å°ä¸åŒ¹é…ï¼Œè¿›è¡Œ resize...")
            student_model.resize_token_embeddings(len(tokenizer))
            logger.info(f"  âœ“ Resized to {len(tokenizer)}")
        else:
            logger.info(f"  âœ“ Embedding å¤§å°æ­£ç¡®")

    if hasattr(student_model, 'lm_head'):
        actual_lmhead_vocab = student_model.lm_head.out_features
        logger.info(f"  lm_head.out_features: {actual_lmhead_vocab}")
        
        if actual_lmhead_vocab != len(tokenizer):
            logger.error(f"  âŒ LM head å¤§å°ä¸åŒ¹é…!")
            raise ValueError("LM head size mismatch!")
    
    # åˆå§‹åŒ–æƒé‡ (ä½¿ç”¨ V2)
    logger.info("ä»æ•™å¸ˆæ¨¡å‹åˆå§‹åŒ–å­¦ç”Ÿæƒé‡...")
    student_model = initialize_student_from_teacher_v2(
        student_model, teacher_model, student_config
    )
    
    logger.info("="*60)
    logger.info("å¤åˆ¶è§†è§‰ç»„ä»¶ï¼ˆä¸å‹ç¼©ï¼‰...")
    logger.info("="*60)

    # å¤åˆ¶vision tower
    if hasattr(teacher_model, 'model') and hasattr(teacher_model.model, 'vision_tower'):
        logger.info("å¤åˆ¶vision tower...")
        student_model.model.vision_tower = teacher_model.model.vision_tower
        # å†»ç»“vision towerå‚æ•°
        for param in student_model.model.vision_tower.parameters():
            param.requires_grad = False
        logger.info("âœ“ Vision towerå·²å¤åˆ¶å¹¶å†»ç»“")
    else:
        logger.error("âœ— æœªæ‰¾åˆ°teacherçš„vision tower!")
        raise RuntimeError("Cannot find vision_tower in teacher model")

    # å¤åˆ¶mm_projector
    if hasattr(teacher_model, 'model') and hasattr(teacher_model.model, 'mm_projector'):
        logger.info("å¤åˆ¶mm_projector...")
        student_model.model.mm_projector = teacher_model.model.mm_projector
        logger.info("âœ“ MM projectorå·²å¤åˆ¶")

    logger.info("="*60)
    student_model.to(device)
    
    # æœ€ç»ˆéªŒè¯
    logger.info("\n" + "=" * 60)
    logger.info("æœ€ç»ˆéªŒè¯...")
    logger.info("=" * 60)
    
    logger.info(f"Tokenizer vocab: {len(tokenizer)}")
    logger.info(f"Teacher embed: {teacher_model.model.embed_tokens.num_embeddings}")
    logger.info(f"Student embed: {student_model.model.embed_tokens.num_embeddings}")
    logger.info(f"Teacher lm_head: {teacher_model.lm_head.out_features}")
    logger.info(f"Student lm_head: {student_model.lm_head.out_features}")
    
    # æµ‹è¯•ä¸€ä¸ªæ ·æœ¬
    logger.info("\næµ‹è¯• embedding lookup...")
    try:
        test_text = "USER: <image>\nWhat is this? ASSISTANT:"
        test_ids = tokenizer(test_text, return_tensors='pt').input_ids
        logger.info(f"  Test IDs shape: {test_ids.shape}")
        logger.info(f"  Test IDs èŒƒå›´: [{test_ids.min().item()}, {test_ids.max().item()}]")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶…èŒƒå›´çš„ token
        max_id = test_ids.max().item()
        vocab_size = len(tokenizer)
        if max_id >= vocab_size:
            logger.error(f"  âŒ Token ID {max_id} >= vocab_size {vocab_size}!")
            raise ValueError("Token ID out of range!")
        
        with torch.no_grad():
            test_embed = student_model.model.embed_tokens(test_ids.to(device))
        logger.info(f"  âœ“ Embedding æˆåŠŸ! shape: {test_embed.shape}")
    except Exception as e:
        logger.error(f"  âŒ Embedding å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise

    # åˆ†æå‹ç¼©
    student_params = sum(p.numel() for p in student_model.parameters())
    logger.info(f"\nå‹ç¼©åˆ†æ:")
    logger.info(f"  æ•™å¸ˆ: {teacher_params/1e9:.2f}B")
    logger.info(f"  å­¦ç”Ÿ: {student_params/1e9:.2f}B")
    logger.info(f"  å‹ç¼©ç‡: {(1 - student_params/teacher_params):.2%}")
    
    # 3. å‡†å¤‡æ•°æ®
    logger.info("\n" + "=" * 60)
    logger.info("å‡†å¤‡æ•°æ®é›†...")
    logger.info("=" * 60)
    
    # å¤„ç†è®­ç»ƒ/éªŒè¯åˆ†å‰²
    if args.val_data_path:
        train_json = args.train_data_path
        val_json = args.val_data_path
        logger.info(f"ä½¿ç”¨æä¾›çš„éªŒè¯é›†")
    elif args.auto_split:
        logger.info(f"è‡ªåŠ¨åˆ†å‰²æ•°æ® ({args.val_split:.1%} éªŒè¯)")
        
        with open(args.train_data_path, 'r') as f:
            all_data = json.load(f)
        
        split_idx = int(len(all_data) * (1 - args.val_split))
        train_data = all_data[:split_idx]
        val_data = all_data[split_idx:]
        
        os.makedirs(args.output_dir, exist_ok=True)
        train_json = os.path.join(args.output_dir, 'train_split.json')
        val_json = os.path.join(args.output_dir, 'val_split.json')
        
        with open(train_json, 'w') as f:
            json.dump(train_data, f)
        with open(val_json, 'w') as f:
            json.dump(val_data, f)
        
        logger.info(f"åˆ†å‰²: {len(train_data)} è®­ç»ƒ, {len(val_data)} éªŒè¯")
    else:
        train_json = args.train_data_path
        val_json = None
        logger.info("ä»…è®­ç»ƒï¼Œæ— éªŒè¯")
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = LLaVADataset(
        train_json,
        args.image_folder,
        tokenizer,
        image_processor,
        max_length=args.max_length,
        is_training=True
    )
    
    val_dataset = None
    if val_json:
        val_dataset = LLaVADataset(
            val_json,
            args.image_folder,
            tokenizer,
            image_processor,
            max_length=args.max_length,
            is_training=False
        )
    
    logger.info(f"âœ“ è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    if val_dataset:
        logger.info(f"âœ“ éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
    
    # æ•°æ®åŠ è½½å™¨
    collator = LLaVACollator(tokenizer, image_processor)
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=collator,
        num_workers=args.num_workers,
        pin_memory=True
    )
    
    val_loader = None
    if val_dataset:
        val_loader = DataLoader(
            val_dataset,
            batch_size=args.batch_size,
            shuffle=False,
            collate_fn=collator,
            num_workers=args.num_workers,
            pin_memory=True
        )
    
    # 4. åˆ›å»ºè®­ç»ƒå™¨ï¼ˆwith RDOï¼‰
    trainer = CIBDTrainerWithRDO(student_model, train_loader, val_loader, args, device)
    
    # æ¢å¤è®­ç»ƒï¼ˆå¦‚æœéœ€è¦ï¼‰
    if args.resume:
        trainer.load_checkpoint(args.resume)
    
    # 5. å¼€å§‹è®­ç»ƒ
    trainer.train()
    
    # 6. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    logger.info("\n" + "=" * 60)
    logger.info("ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    logger.info("=" * 60)
    
    final_path = os.path.join(args.output_dir, 'final_model')
    student_model.save_pretrained(final_path)
    tokenizer.save_pretrained(final_path)
    
    logger.info(f"âœ“ æœ€ç»ˆæ¨¡å‹: {final_path}")
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    student_params = sum(p.numel() for p in student_model.parameters())
    stats = {
        'teacher_params': teacher_params,
        'student_params': student_params,
        'compression_ratio': 1 - (student_params / teacher_params),
        'config': student_config.to_dict(),
        'final_beta': trainer.rdo.beta,
        'rdo_config': {
            'initial_beta': args.initial_beta,
            'target_compression_rate': args.target_compression_rate,
            'beta_adaptation_rate': args.beta_adaptation_rate,
        }
    }
    
    with open(os.path.join(args.output_dir, 'compression_stats.json'), 'w') as f:
        json.dump(stats, f, indent=2)
    
    logger.info("\n" + "=" * 60)
    logger.info("âœ“ è®­ç»ƒå®Œæˆ!")
    logger.info(f"âœ“ æœ€ç»ˆ Î²: {trainer.rdo.beta:.4f}")
    logger.info("=" * 60)


if __name__ == "__main__":
    main()
