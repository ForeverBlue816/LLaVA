#!/usr/bin/env python3
"""
CIBDè®­ç»ƒè„šæœ¬ - V2 with RDO (Multi-GPU)
æ”¯æŒï¼š
1. DataParallel (DP)
2. DistributedDataParallel (DDP) 
3. DeepSpeed
"""

import os
import sys
import json
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.utils.data import DataLoader, DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
from transformers import get_cosine_schedule_with_warmup
import argparse
from tqdm import tqdm
import logging
from typing import Optional, Dict, Any
import numpy as np

# å¯¼å…¥LLaVAç»„ä»¶
from llava.model.builder import load_pretrained_model
from llava.model.language_model.llava_llama import LlavaLlamaForCausalLM
from transformers.modeling_outputs import CausalLMOutputWithPast

# å¯¼å…¥CIBDç»„ä»¶
from cibd.dataset import LLaVADataset, LLaVACollator
from cibd.llava_cibd_model_v2 import LlavaCIBDModelV2
from cibd.create_student_model_v2 import (
    create_compressed_student_config_v2,
    initialize_student_from_teacher_v2,
    estimate_model_params
)

# å¯¼å…¥RDOç»„ä»¶
from cibd.rate_distortion_optimizer import (
    RateDistortionOptimizer,
    AdaptiveRateScheduler
)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    format='%(asctime)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)


def setup_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ["RANK"])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ['LOCAL_RANK'])
    elif 'SLURM_PROCID' in os.environ:
        # SLURM environment
        rank = int(os.environ['SLURM_PROCID'])
        world_size = int(os.environ['SLURM_NTASKS'])
        local_rank = rank % torch.cuda.device_count()
    else:
        rank = 0
        world_size = 1
        local_rank = 0
    
    if world_size > 1:
        torch.cuda.set_device(local_rank)
        dist.init_process_group(
            backend='nccl',
            init_method='env://',
            world_size=world_size,
            rank=rank
        )
        dist.barrier()
    
    return rank, world_size, local_rank


def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()


class CIBDTrainerMultiGPU:
    """
    CIBDè®­ç»ƒå™¨ - å¤šGPUç‰ˆæœ¬
    
    æ”¯æŒï¼š
    1. DataParallel (DP) - ç®€å•å¤šå¡
    2. DistributedDataParallel (DDP) - æ¨èå¤šå¡
    3. DeepSpeed - å¤§æ¨¡å‹ä¼˜åŒ–
    """
    
    def __init__(self, model, train_loader, val_loader, args, device, 
                 rank=0, world_size=1, local_rank=0):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.args = args
        self.device = device
        self.rank = rank
        self.world_size = world_size
        self.local_rank = local_rank
        self.is_main_process = (rank == 0)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=args.learning_rate,
            weight_decay=args.weight_decay
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        num_training_steps = len(train_loader) * args.num_epochs
        num_warmup_steps = int(num_training_steps * args.warmup_ratio)
        
        self.scheduler = get_cosine_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=num_warmup_steps,
            num_training_steps=num_training_steps
        )
        
        # RDO ä¼˜åŒ–å™¨ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
        if self.is_main_process:
            self.rdo = RateDistortionOptimizer(
                initial_beta=args.initial_beta,
                target_compression=args.target_compression_rate,
                adaptation_rate=args.beta_adaptation_rate,
                window_size=100
            )
        else:
            self.rdo = None
        
        # è‡ªé€‚åº”å‹ç¼©ç‡è°ƒåº¦å™¨
        if args.use_adaptive_compression:
            self.compression_scheduler = AdaptiveRateScheduler(
                initial_rate=args.initial_compression_rate,
                target_rate=args.target_compression_rate,
                warmup_steps=num_warmup_steps,
                total_steps=num_training_steps
            )
        else:
            self.compression_scheduler = None
        
        # è®­ç»ƒçŠ¶æ€
        self.global_step = 0
        self.current_epoch = 0
        self.best_val_loss = float('inf')
        
        # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
        if self.is_main_process:
            os.makedirs(args.output_dir, exist_ok=True)
            self.checkpoint_dir = os.path.join(args.output_dir, 'checkpoints')
            os.makedirs(self.checkpoint_dir, exist_ok=True)
            
            # æ—¥å¿—æ–‡ä»¶
            self.log_file = os.path.join(args.output_dir, 'training_log.txt')
            self.rdo_log_file = os.path.join(args.output_dir, 'rdo_log.txt')
        
        if self.is_main_process:
            logger.info("\n" + "=" * 60)
            logger.info("è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆï¼ˆMulti-GPU with RDOï¼‰")
            logger.info("=" * 60)
            logger.info(f"World Size: {world_size}")
            logger.info(f"Rank: {rank}")
            logger.info(f"Local Rank: {local_rank}")
            logger.info(f"è®­ç»ƒæ ·æœ¬: {len(train_loader.dataset)}")
            logger.info(f"éªŒè¯æ ·æœ¬: {len(val_loader.dataset) if val_loader else 0}")
            logger.info(f"æ€»æ­¥æ•°: {num_training_steps}")
            logger.info(f"é¢„çƒ­æ­¥æ•°: {num_warmup_steps}")
            logger.info(f"å­¦ä¹ ç‡: {args.learning_rate}")
            logger.info(f"æ‰¹æ¬¡å¤§å°: {args.batch_size} (per GPU)")
            logger.info(f"å…¨å±€æ‰¹æ¬¡: {args.batch_size * world_size}")
            logger.info(f"æ¢¯åº¦ç´¯ç§¯: {args.gradient_accumulation_steps}")
            logger.info(f"\nğŸ”§ RDO é…ç½®:")
            logger.info(f"  åˆå§‹ Î²: {args.initial_beta}")
            logger.info(f"  ç›®æ ‡å‹ç¼©ç‡: {args.target_compression_rate}")
            logger.info(f"  Î² è‡ªé€‚åº”ç‡: {args.beta_adaptation_rate}")
            logger.info("=" * 60 + "\n")
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        if self.is_main_process:
            logger.info("\n" + "=" * 60)
            logger.info("å¼€å§‹è®­ç»ƒï¼ˆMulti-GPU with RDOï¼‰")
            logger.info("=" * 60 + "\n")
        
        for epoch in range(self.current_epoch, self.args.num_epochs):
            self.current_epoch = epoch
            
            # è®¾ç½®åˆ†å¸ƒå¼é‡‡æ ·å™¨çš„epochï¼ˆç¡®ä¿æ¯ä¸ªepochæ•°æ®shuffleä¸åŒï¼‰
            if hasattr(self.train_loader.sampler, 'set_epoch'):
                self.train_loader.sampler.set_epoch(epoch)
            
            if self.is_main_process:
                logger.info(f"\n{'='*60}")
                logger.info(f"Epoch {epoch + 1}/{self.args.num_epochs}")
                logger.info(f"{'='*60}")
            
            # è®­ç»ƒä¸€ä¸ªepoch
            train_metrics = self._train_epoch()
            
            # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
            if self.world_size > 1:
                dist.barrier()
            
            # è®°å½•è®­ç»ƒæŒ‡æ ‡ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
            if self.is_main_process:
                self._log_metrics(epoch, train_metrics, phase='train')
            
            # éªŒè¯
            if self.val_loader is not None:
                val_metrics = self._validate()
                
                if self.is_main_process:
                    self._log_metrics(epoch, val_metrics, phase='val')
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    if val_metrics['total_loss'] < self.best_val_loss:
                        self.best_val_loss = val_metrics['total_loss']
                        self._save_checkpoint('best_model')
                        logger.info(f"âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (val_loss={self.best_val_loss:.4f})")
            
            # å®šæœŸä¿å­˜checkpoint
            if self.is_main_process and (epoch + 1) % self.args.save_steps == 0:
                self._save_checkpoint(f'checkpoint_epoch_{epoch+1}')
            
            # ä¿å­˜Paretoå‰æ²¿
            if self.is_main_process:
                self._save_pareto_frontier(epoch)
            
            # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
            if self.world_size > 1:
                dist.barrier()
        
        if self.is_main_process:
            logger.info("\n" + "=" * 60)
            logger.info("è®­ç»ƒå®Œæˆ!")
            logger.info("=" * 60 + "\n")
    
    def _train_epoch(self):
        """è®­ç»ƒä¸€ä¸ªepoch - æœ€ç»ˆä¿®å¤ç‰ˆ"""
        self.model.train()
        
        total_loss = 0
        loss_components = {
            'task_loss': 0,
            'kl_loss': 0,
            'feature_loss': 0,
            'ib_loss': 0,
            'layer_0_loss': 0,
            'layer_1_loss': 0,
            'layer_2_loss': 0,
        }
        
        # RDO ç»Ÿè®¡ï¼ˆåªåœ¨ä¸»è¿›ç¨‹æ”¶é›†ï¼‰
        rdo_stats = {
            'beta_values': [],
            'compression_rates': [],
            'rate_values': [],
            'distortion_values': []
        } if self.is_main_process else None
        
        self.optimizer.zero_grad()
        
        # åªåœ¨ä¸»è¿›ç¨‹æ˜¾ç¤ºè¿›åº¦æ¡
        if self.is_main_process:
            progress_bar = tqdm(
                self.train_loader,
                desc=f"Epoch {self.current_epoch + 1}",
                dynamic_ncols=True
            )
        else:
            progress_bar = self.train_loader
        
        for step, batch in enumerate(progress_bar):
            try:
                # ===== ã€æœ€ç»ˆä¿®å¤ã€‘å¤„ç† images list =====
                images = batch.pop('images', None)
                
                # å…¶ä»–å­—æ®µç§»åˆ°è®¾å¤‡
                batch = {k: v.to(self.device) if torch.is_tensor(v) else v 
                        for k, v in batch.items()}
                
                # å¤„ç† images
                if images is not None:
                    if isinstance(images, list):
                        # è¿‡æ»¤å¹¶å¤„ç† list
                        processed_images = []
                        valid_count = 0
                        
                        for img in images:
                            if img is not None and torch.is_tensor(img):
                                # å…œåº•æ£€æŸ¥ï¼šç¡®ä¿æ˜¯ 3 é€šé“
                                if img.dim() == 3:
                                    if img.shape[0] == 4:
                                        img = img[:3, :, :]
                                        if self.is_main_process and valid_count == 0:
                                            logger.warning(f"Step {step}: å‘ç°4é€šé“å›¾ç‰‡ï¼Œå·²è£å‰ª")
                                    elif img.shape[0] != 3:
                                        if self.is_main_process:
                                            logger.error(f"Step {step}: å¼‚å¸¸é€šé“æ•° {img.shape[0]}")
                                        raise ValueError(f"Invalid channel count: {img.shape[0]}")
                                
                                processed_images.append(img.to(self.device))
                                valid_count += 1
                            else:
                                processed_images.append(None)
                        
                        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆå›¾ç‰‡ï¼Œè·³è¿‡
                        if valid_count == 0:
                            if self.is_main_process:
                                logger.warning(f"Step {step}: æ²¡æœ‰æœ‰æ•ˆå›¾ç‰‡ï¼Œè·³è¿‡")
                            self.optimizer.zero_grad()
                            continue
                        
                        # æ ¹æ®æƒ…å†µè½¬æ¢æ ¼å¼
                        if all(img is not None for img in processed_images):
                            images = torch.stack(processed_images)
                        else:
                            images = processed_images
                        
                    elif torch.is_tensor(images):
                        if images.dim() == 4 and images.shape[1] == 4:
                            images = images[:, :3, :, :]
                            if self.is_main_process:
                                logger.warning(f"Step {step}: å‘ç°4é€šé“æ‰¹æ¬¡ï¼Œå·²è£å‰ª")
                        elif images.dim() == 4 and images.shape[1] != 3:
                            if self.is_main_process:
                                logger.error(f"Step {step}: å¼‚å¸¸é€šé“æ•°æ‰¹æ¬¡ {images.shape[1]}")
                            raise ValueError(f"Invalid channel count: {images.shape[1]}")
                        
                        images = images.to(self.device)
                
                batch['images'] = images
                # ===== ã€ä¿®å¤ç»“æŸã€‘=====
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(
                    input_ids=batch['input_ids'],
                    attention_mask=batch['attention_mask'],
                    labels=batch['labels'],
                    images=batch['images'],
                    return_dict=True
                )
                
                # è·å–æŸå¤±
                loss = outputs.loss
                
                # RDO ä¼˜åŒ–
                if hasattr(outputs, 'loss_info'):
                    task_loss = outputs.loss_info.get('task_loss', 0)
                    ib_loss = outputs.loss_info.get('ib_loss', 0)
                    
                    if ib_loss > 0 and task_loss > 0:
                        if self.is_main_process:
                            rate = outputs.loss_info.get('ib_kl', ib_loss)
                            distortion = task_loss
                            
                            optimal_beta = self.rdo.step(
                                torch.tensor(distortion).to(self.device),
                                torch.tensor(rate).to(self.device)
                            )
                            
                            rdo_stats['beta_values'].append(optimal_beta)
                            rdo_stats['rate_values'].append(rate)
                            rdo_stats['distortion_values'].append(distortion)
                        else:
                            optimal_beta = 1.0
                        
                        if self.world_size > 1:
                            beta_tensor = torch.tensor(optimal_beta, device=self.device)
                            dist.broadcast(beta_tensor, src=0)
                            optimal_beta = beta_tensor.item()
                        
                        model_to_update = self.model.module if hasattr(self.model, 'module') else self.model
                        if hasattr(model_to_update, 'visual_ib') and hasattr(model_to_update.visual_ib, 'log_beta'):
                            with torch.no_grad():
                                model_to_update.visual_ib.log_beta.data = torch.tensor(
                                    np.log(optimal_beta),
                                    dtype=torch.float32,
                                    device=self.device
                                )
                
                # æ¢¯åº¦ç´¯ç§¯
                loss = loss / self.args.gradient_accumulation_steps
                loss.backward()
                
                # ç´¯ç§¯æŸå¤±
                total_loss += loss.item() * self.args.gradient_accumulation_steps
                
                # ç´¯ç§¯æŸå¤±ç»„ä»¶
                if hasattr(outputs, 'loss_info'):
                    for key in loss_components:
                        if key in outputs.loss_info:
                            loss_components[key] += outputs.loss_info[key]
                
                # æ›´æ–°å‚æ•°
                if (step + 1) % self.args.gradient_accumulation_steps == 0:
                    if self.args.max_grad_norm > 0:
                        torch.nn.utils.clip_grad_norm_(
                            self.model.parameters(),
                            self.args.max_grad_norm
                        )
                    
                    self.optimizer.step()
                    self.scheduler.step()
                    self.optimizer.zero_grad()
                    self.global_step += 1
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    if self.is_main_process:
                        current_lr = self.scheduler.get_last_lr()[0]
                        current_beta = rdo_stats['beta_values'][-1] if rdo_stats['beta_values'] else 1.0
                        progress_bar.set_postfix({
                            'loss': f'{loss.item():.4f}',
                            'lr': f'{current_lr:.2e}',
                            'Î²': f'{current_beta:.3f}'
                        })
            
            except Exception as e:
                if self.is_main_process:
                    logger.error(f"Error in training step {step}: {e}")
                    import traceback
                    traceback.print_exc()
                
                self.optimizer.zero_grad()
                continue
        
        # èšåˆæ‰€æœ‰GPUçš„æŸå¤±
        if self.world_size > 1:
            total_loss_tensor = torch.tensor(total_loss, device=self.device)
            dist.all_reduce(total_loss_tensor, op=dist.ReduceOp.AVG)
            total_loss = total_loss_tensor.item()
            
            for key in loss_components:
                loss_tensor = torch.tensor(loss_components[key], device=self.device)
                dist.all_reduce(loss_tensor, op=dist.ReduceOp.AVG)
                loss_components[key] = loss_tensor.item()
        
        # è®¡ç®—å¹³å‡
        num_batches = len(self.train_loader)
        metrics = {
            'total_loss': total_loss / num_batches,
            **{k: v / num_batches for k, v in loss_components.items()}
        }
        
        # æ·»åŠ  RDO ç»Ÿè®¡
        if self.is_main_process and rdo_stats and rdo_stats['beta_values']:
            metrics['avg_beta'] = np.mean(rdo_stats['beta_values'])
            metrics['std_beta'] = np.std(rdo_stats['beta_values'])
            metrics['avg_rate'] = np.mean(rdo_stats['rate_values'])
            metrics['avg_distortion'] = np.mean(rdo_stats['distortion_values'])
            
            if rdo_stats['compression_rates']:
                metrics['avg_compression_rate'] = np.mean(rdo_stats['compression_rates'])
            
            self._log_rdo_stats(rdo_stats)
        
        return metrics
    
    def _validate(self):
        """éªŒè¯"""
        self.model.eval()
        
        total_loss = 0
        loss_components = {
            'task_loss': 0,
            'kl_loss': 0,
            'feature_loss': 0,
            'ib_loss': 0,
            'layer_0_loss': 0,
            'layer_1_loss': 0,
            'layer_2_loss': 0,
        }
        
        with torch.no_grad():
            val_iter = tqdm(self.val_loader, desc="Validation", dynamic_ncols=True) if self.is_main_process else self.val_loader
            
            for batch in val_iter:
                # ç§»åŠ¨åˆ°è®¾å¤‡
                batch = {k: v.to(self.device) if torch.is_tensor(v) else v 
                        for k, v in batch.items()}
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(
                    input_ids=batch['input_ids'],
                    attention_mask=batch['attention_mask'],
                    labels=batch['labels'],
                    images=batch.get('images'),
                    image_sizes=batch.get('image_sizes'),  # â† æ·»åŠ è¿™ä¸ª
                    return_dict=True
                )
                loss = outputs.loss
                
                total_loss += loss.item()
                
                # ç´¯ç§¯æŸå¤±ç»„ä»¶
                if hasattr(outputs, 'loss_info'):
                    for key in loss_components:
                        if key in outputs.loss_info:
                            loss_components[key] += outputs.loss_info[key]
        
        # èšåˆæ‰€æœ‰GPUçš„æŸå¤±
        if self.world_size > 1:
            total_loss_tensor = torch.tensor(total_loss, device=self.device)
            dist.all_reduce(total_loss_tensor, op=dist.ReduceOp.AVG)
            total_loss = total_loss_tensor.item()
            
            for key in loss_components:
                loss_tensor = torch.tensor(loss_components[key], device=self.device)
                dist.all_reduce(loss_tensor, op=dist.ReduceOp.AVG)
                loss_components[key] = loss_tensor.item()
        
        # è®¡ç®—å¹³å‡
        num_batches = len(self.val_loader)
        metrics = {
            'total_loss': total_loss / num_batches,
            **{k: v / num_batches for k, v in loss_components.items()}
        }
        
        return metrics
    
    def _log_metrics(self, epoch, metrics, phase='train'):
        """è®°å½•æŒ‡æ ‡ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰"""
        if not self.is_main_process:
            return
        
        log_str = f"\n[{phase.upper()}] Epoch {epoch + 1}\n"
        log_str += f"  Total Loss: {metrics['total_loss']:.4f}\n"
        log_str += f"  Task Loss: {metrics['task_loss']:.4f}\n"
        log_str += f"  KL Loss: {metrics['kl_loss']:.4f}\n"
        log_str += f"  Feature Loss: {metrics['feature_loss']:.4f}\n"
        log_str += f"  IB Loss: {metrics['ib_loss']:.4f}\n"
        
        # RDO æŒ‡æ ‡
        if 'avg_beta' in metrics:
            log_str += f"\n  ğŸ”§ RDO Stats:\n"
            log_str += f"    Avg Î²: {metrics['avg_beta']:.4f} Â± {metrics['std_beta']:.4f}\n"
            log_str += f"    Avg Rate: {metrics['avg_rate']:.4f}\n"
            log_str += f"    Avg Distortion: {metrics['avg_distortion']:.4f}\n"
        
        if 'avg_compression_rate' in metrics:
            log_str += f"    Compression Rate: {metrics['avg_compression_rate']:.4f}\n"
        
        logger.info(log_str)
        
        # å†™å…¥æ–‡ä»¶
        with open(self.log_file, 'a') as f:
            f.write(log_str + "\n")
    
    def _log_rdo_stats(self, rdo_stats):
        """è®°å½• RDO è¯¦ç»†ç»Ÿè®¡"""
        if not self.is_main_process or not rdo_stats['beta_values']:
            return
        
        log_str = f"\nStep {self.global_step} RDO Stats:\n"
        log_str += f"  Î²: {np.mean(rdo_stats['beta_values']):.4f}\n"
        log_str += f"  Rate: {np.mean(rdo_stats['rate_values']):.4f}\n"
        log_str += f"  Distortion: {np.mean(rdo_stats['distortion_values']):.4f}\n"
        
        with open(self.rdo_log_file, 'a') as f:
            f.write(log_str)
    
    def _save_pareto_frontier(self, epoch):
        """ä¿å­˜Paretoå‰æ²¿"""
        if not self.is_main_process or self.rdo is None:
            return
        
        rates, distortions = self.rdo.get_pareto_frontier()
        
        if rates:
            pareto_data = {
                'epoch': epoch,
                'rates': rates,
                'distortions': distortions
            }
            
            pareto_file = os.path.join(
                self.args.output_dir, 
                f'pareto_frontier_epoch_{epoch}.json'
            )
            
            with open(pareto_file, 'w') as f:
                json.dump(pareto_data, f, indent=2)
            
            logger.info(f"âœ“ Saved Pareto frontier: {len(rates)} points")
    
    def _save_checkpoint(self, name):
        """ä¿å­˜checkpointï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰"""
        if not self.is_main_process:
            return
        
        checkpoint_path = os.path.join(self.checkpoint_dir, name)
        os.makedirs(checkpoint_path, exist_ok=True)
        
        # è·å–å®é™…çš„æ¨¡å‹ï¼ˆå»é™¤DDP wrapperï¼‰
        model_to_save = self.model.module if hasattr(self.model, 'module') else self.model
        
        # ä¿å­˜æ¨¡å‹
        model_to_save.save_pretrained(checkpoint_path)
        
        # ä¿å­˜è®­ç»ƒçŠ¶æ€
        state = {
            'epoch': self.current_epoch,
            'global_step': self.global_step,
            'best_val_loss': self.best_val_loss,
            'optimizer': self.optimizer.state_dict(),
            'scheduler': self.scheduler.state_dict(),
        }
        
        if self.rdo is not None:
            state['rdo_beta'] = self.rdo.beta
            state['rdo_history'] = {
                'rate_history': list(self.rdo.rate_history),
                'distortion_history': list(self.rdo.distortion_history),
                'beta_history': list(self.rdo.beta_history),
            }
        
        if self.compression_scheduler is not None:
            state['compression_scheduler_step'] = self.compression_scheduler.current_step
        
        torch.save(state, os.path.join(checkpoint_path, 'trainer_state.pt'))
        
        logger.info(f"âœ“ Checkpoint saved: {checkpoint_path}")
    
    def load_checkpoint(self, checkpoint_path):
        """åŠ è½½checkpoint"""
        if self.is_main_process:
            logger.info(f"åŠ è½½checkpoint: {checkpoint_path}")
        
        # åŠ è½½è®­ç»ƒçŠ¶æ€
        state_path = os.path.join(checkpoint_path, 'trainer_state.pt')
        if os.path.exists(state_path):
            state = torch.load(state_path, map_location=self.device)
            self.current_epoch = state['epoch']
            self.global_step = state['global_step']
            self.best_val_loss = state['best_val_loss']
            self.optimizer.load_state_dict(state['optimizer'])
            self.scheduler.load_state_dict(state['scheduler'])
            
            # æ¢å¤ RDO çŠ¶æ€ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
            if self.is_main_process and self.rdo is not None:
                if 'rdo_beta' in state:
                    self.rdo.beta = state['rdo_beta']
                
                if 'rdo_history' in state:
                    from collections import deque
                    self.rdo.rate_history = deque(state['rdo_history']['rate_history'], maxlen=100)
                    self.rdo.distortion_history = deque(state['rdo_history']['distortion_history'], maxlen=100)
                    self.rdo.beta_history = deque(state['rdo_history']['beta_history'], maxlen=100)
            
            if 'compression_scheduler_step' in state and self.compression_scheduler is not None:
                self.compression_scheduler.current_step = state['compression_scheduler_step']
            
            if self.is_main_process:
                logger.info(f"âœ“ ä»epoch {self.current_epoch} æ¢å¤è®­ç»ƒ")
                if self.rdo is not None:
                    logger.info(f"âœ“ RDO Î² æ¢å¤ä¸º: {self.rdo.beta:.4f}")


def parse_args():
    parser = argparse.ArgumentParser(description="CIBD Training Script V2 with RDO (Multi-GPU)")
    
    # åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°
    parser.add_argument("--distributed_backend", type=str, default="ddp",
                       choices=["dp", "ddp", "deepspeed"],
                       help="åˆ†å¸ƒå¼è®­ç»ƒåç«¯: dp (DataParallel), ddp (DistributedDataParallel), deepspeed")
    parser.add_argument("--local_rank", type=int, default=-1,
                       help="DDP: local rank (è‡ªåŠ¨è®¾ç½®)")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--teacher_model_path", type=str, required=True)
    parser.add_argument("--compression_ratio", type=float, default=0.5)
    parser.add_argument("--compress_hidden", action="store_true")
    parser.add_argument("--compress_heads", action="store_true")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--train_data_path", type=str, required=True)
    parser.add_argument("--val_data_path", type=str, default=None)
    parser.add_argument("--image_folder", type=str, required=True)
    parser.add_argument("--auto_split", action="store_true")
    parser.add_argument("--val_split", type=float, default=0.1)
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--batch_size", type=int, default=4,
                       help="æ¯ä¸ªGPUçš„batch size")
    parser.add_argument("--num_epochs", type=int, default=3)
    parser.add_argument("--learning_rate", type=float, default=2e-5)
    parser.add_argument("--weight_decay", type=float, default=0.01)
    parser.add_argument("--warmup_ratio", type=float, default=0.03)
    parser.add_argument("--max_grad_norm", type=float, default=1.0)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1)
    
    # RDO å‚æ•°
    parser.add_argument("--initial_beta", type=float, default=1.0)
    parser.add_argument("--target_compression_rate", type=float, default=0.5)
    parser.add_argument("--beta_adaptation_rate", type=float, default=0.01)
    parser.add_argument("--use_adaptive_compression", action="store_true")
    parser.add_argument("--initial_compression_rate", type=float, default=0.1)
    
    # æ•°æ®åŠ è½½
    parser.add_argument("--num_workers", type=int, default=4)
    parser.add_argument("--max_length", type=int, default=2048)
    
    # ä¿å­˜å’Œæ—¥å¿—
    parser.add_argument("--output_dir", type=str, required=True)
    parser.add_argument("--save_steps", type=int, default=1)
    parser.add_argument("--resume", type=str, default=None)
    
    return parser.parse_args()


def main():
    args = parse_args()
    
    # è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒ
    rank, world_size, local_rank = setup_distributed()
    
    # è®¾ç½®è®¾å¤‡
    if world_size > 1:
        device = torch.device(f"cuda:{local_rank}")
        torch.cuda.set_device(device)
    else:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    is_main_process = (rank == 0)
    
    if is_main_process:
        logger.info("\n" + "=" * 60)
        logger.info("CIBD Training V2 with RDO (Multi-GPU)")
        logger.info("=" * 60)
        logger.info(f"World Size: {world_size}")
        logger.info(f"Rank: {rank}")
        logger.info(f"Local Rank: {local_rank}")
        logger.info(f"Device: {device}")
        logger.info(f"Backend: {args.distributed_backend}")
        logger.info(f"Batch size per GPU: {args.batch_size}")
        logger.info(f"Global batch size: {args.batch_size * world_size}")
        logger.info("=" * 60 + "\n")
    
    # 1. åŠ è½½æ•™å¸ˆæ¨¡å‹ï¼ˆåªåœ¨ä¸»è¿›ç¨‹æ‰“å°æ—¥å¿—ï¼‰
    if is_main_process:
        logger.info("\n" + "=" * 60)
        logger.info("åŠ è½½æ•™å¸ˆæ¨¡å‹...")
        logger.info("=" * 60)
    
    tokenizer, teacher_model, image_processor, context_len = load_pretrained_model(
        model_path=args.teacher_model_path,
        model_base=None,
        model_name=args.teacher_model_path.split('/')[-1],
        device_map=None
    )
    
    if is_main_process:
        logger.info(f"âœ“ Image processor ç±»å‹: {type(image_processor)}")
        logger.info(f"  Size: {getattr(image_processor, 'size', 'N/A')}")
        logger.info(f"  Crop size: {getattr(image_processor, 'crop_size', 'N/A')}")
    # ç¡®ä¿vocabå¤§å°æ­£ç¡®
    if is_main_process:
        logger.info(f"Tokenizer vocab size: {len(tokenizer)}")
    
    if hasattr(teacher_model.config, 'vocab_size'):
        tokenizer_vocab_size = len(tokenizer)
        if teacher_model.config.vocab_size != tokenizer_vocab_size:
            teacher_model.config.vocab_size = tokenizer_vocab_size
            if is_main_process:
                logger.info(f"  âœ“ å·²æ›´æ–° vocab_size ä¸º: {tokenizer_vocab_size}")
    
    teacher_model.to(device)
    teacher_model.eval()
    
    if is_main_process:
        teacher_params = sum(p.numel() for p in teacher_model.parameters())
        logger.info(f"âœ“ æ•™å¸ˆæ¨¡å‹: {teacher_params/1e6:.2f}M å‚æ•°")
    
    # 2. åˆ›å»ºå­¦ç”Ÿæ¨¡å‹
    if is_main_process:
        logger.info("\n" + "=" * 60)
        logger.info("åˆ›å»ºå­¦ç”Ÿæ¨¡å‹...")
        logger.info("=" * 60)
    
    teacher_config = teacher_model.config
    student_config = create_compressed_student_config_v2(
        teacher_config,
        compression_ratio=args.compression_ratio,
        compress_hidden=args.compress_hidden,
        compress_heads=args.compress_heads
    )
    
    student_model = LlavaCIBDModelV2(student_config, teacher_model)
    
    # Embedding æ£€æŸ¥
    if hasattr(student_model.model, 'embed_tokens'):
        actual_embed_vocab = student_model.model.embed_tokens.num_embeddings
        if actual_embed_vocab != len(tokenizer):
            student_model.resize_token_embeddings(len(tokenizer))
            if is_main_process:
                logger.info(f"  âœ“ Embedding resized to {len(tokenizer)}")
    
    # åˆå§‹åŒ–æƒé‡
    if is_main_process:
        logger.info("ä»æ•™å¸ˆæ¨¡å‹åˆå§‹åŒ–å­¦ç”Ÿæƒé‡...")
    
    student_model = initialize_student_from_teacher_v2(
        student_model, teacher_model, student_config
    )
    
    # å¤åˆ¶è§†è§‰ç»„ä»¶
    if is_main_process:
        logger.info("å¤åˆ¶è§†è§‰ç»„ä»¶...")
    
    if hasattr(teacher_model, 'model') and hasattr(teacher_model.model, 'vision_tower'):
        student_model.model.vision_tower = teacher_model.model.vision_tower
        for param in student_model.model.vision_tower.parameters():
            param.requires_grad = False
        if is_main_process:
            logger.info("âœ“ Vision towerå·²å¤åˆ¶å¹¶å†»ç»“")
    
    if hasattr(teacher_model, 'model') and hasattr(teacher_model.model, 'mm_projector'):
        student_model.model.mm_projector = teacher_model.model.mm_projector
        if is_main_process:
            logger.info("âœ“ MM projectorå·²å¤åˆ¶")
    
    # å°†æ¨¡å‹ç§»åˆ°è®¾å¤‡
    student_model.to(device)
    student_model = student_model.float()
    # åŒ…è£…æ¨¡å‹ï¼ˆå¤šGPUï¼‰
    if world_size > 1:
        if args.distributed_backend == "ddp":
            student_model = DDP(
                student_model,
                device_ids=[local_rank],
                output_device=local_rank,
                find_unused_parameters=True  # å› ä¸ºæœ‰äº›å‚æ•°è¢«å†»ç»“
            )
            if is_main_process:
                logger.info("âœ“ ä½¿ç”¨ DistributedDataParallel")
        elif args.distributed_backend == "dp":
            student_model = nn.DataParallel(student_model)
            if is_main_process:
                logger.info("âœ“ ä½¿ç”¨ DataParallel")
    else:
        if is_main_process:
            logger.info("âœ“ ä½¿ç”¨å•GPUè®­ç»ƒ")
    
    if is_main_process:
        student_params = sum(p.numel() for p in student_model.parameters())
        logger.info(f"\nå‹ç¼©åˆ†æ:")
        logger.info(f"  æ•™å¸ˆ: {teacher_params/1e9:.2f}B")
        logger.info(f"  å­¦ç”Ÿ: {student_params/1e9:.2f}B")
        logger.info(f"  å‹ç¼©ç‡: {(1 - student_params/teacher_params):.2%}")
    
    # 3. å‡†å¤‡æ•°æ®
    if is_main_process:
        logger.info("\n" + "=" * 60)
        logger.info("å‡†å¤‡æ•°æ®é›†...")
        logger.info("=" * 60)
    
    # å¤„ç†æ•°æ®åˆ†å‰²
    if args.val_data_path:
        train_json = args.train_data_path
        val_json = args.val_data_path
    elif args.auto_split:
        with open(args.train_data_path, 'r') as f:
            all_data = json.load(f)
        
        split_idx = int(len(all_data) * (1 - args.val_split))
        train_data = all_data[:split_idx]
        val_data = all_data[split_idx:]
        
        if is_main_process:
            os.makedirs(args.output_dir, exist_ok=True)
            train_json = os.path.join(args.output_dir, 'train_split.json')
            val_json = os.path.join(args.output_dir, 'val_split.json')
            
            with open(train_json, 'w') as f:
                json.dump(train_data, f)
            with open(val_json, 'w') as f:
                json.dump(val_data, f)
            
            logger.info(f"åˆ†å‰²: {len(train_data)} è®­ç»ƒ, {len(val_data)} éªŒè¯")
        
        # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
        if world_size > 1:
            dist.barrier()
            # éä¸»è¿›ç¨‹ä¹Ÿéœ€è¦çŸ¥é“æ–‡ä»¶è·¯å¾„
            if not is_main_process:
                train_json = os.path.join(args.output_dir, 'train_split.json')
                val_json = os.path.join(args.output_dir, 'val_split.json')
    else:
        train_json = args.train_data_path
        val_json = None
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = LLaVADataset(
        train_json,
        args.image_folder,
        tokenizer,
        image_processor,
        max_length=args.max_length,
        is_training=True
    )
    
    val_dataset = None
    if val_json:
        val_dataset = LLaVADataset(
            val_json,
            args.image_folder,
            tokenizer,
            image_processor,
            max_length=args.max_length,
            is_training=False
        )
    
    if is_main_process:
        logger.info(f"âœ“ è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
        if val_dataset:
            logger.info(f"âœ“ éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
    
    # æ•°æ®åŠ è½½å™¨ï¼ˆä½¿ç”¨DistributedSamplerï¼‰
    collator = LLaVACollator(tokenizer, image_processor)
    
    if world_size > 1 and args.distributed_backend == "ddp":
        train_sampler = DistributedSampler(
            train_dataset,
            num_replicas=world_size,
            rank=rank,
            shuffle=True
        )
        shuffle = False
    else:
        train_sampler = None
        shuffle = True
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=shuffle,
        sampler=train_sampler,
        collate_fn=collator,
        num_workers=args.num_workers,
        pin_memory=True
    )
    
    val_loader = None
    if val_dataset:
        if world_size > 1 and args.distributed_backend == "ddp":
            val_sampler = DistributedSampler(
                val_dataset,
                num_replicas=world_size,
                rank=rank,
                shuffle=False
            )
        else:
            val_sampler = None
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=args.batch_size,
            shuffle=False,
            sampler=val_sampler,
            collate_fn=collator,
            num_workers=args.num_workers,
            pin_memory=True
        )
    
    # 4. åˆ›å»ºè®­ç»ƒå™¨
    trainer = CIBDTrainerMultiGPU(
        student_model, train_loader, val_loader, args, device,
        rank=rank, world_size=world_size, local_rank=local_rank
    )
    
    # æ¢å¤è®­ç»ƒ
    if args.resume:
        trainer.load_checkpoint(args.resume)
    
    # 5. å¼€å§‹è®­ç»ƒ
    trainer.train()
    
    # 6. ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
    if is_main_process:
        logger.info("\n" + "=" * 60)
        logger.info("ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
        logger.info("=" * 60)
        
        final_path = os.path.join(args.output_dir, 'final_model')
        
        # è·å–å®é™…çš„æ¨¡å‹
        model_to_save = student_model.module if hasattr(student_model, 'module') else student_model
        model_to_save.save_pretrained(final_path)
        tokenizer.save_pretrained(final_path)
        
        logger.info(f"âœ“ æœ€ç»ˆæ¨¡å‹: {final_path}")
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        student_params = sum(p.numel() for p in model_to_save.parameters())
        stats = {
            'teacher_params': teacher_params,
            'student_params': student_params,
            'compression_ratio': 1 - (student_params / teacher_params),
            'config': student_config.to_dict(),
            'final_beta': trainer.rdo.beta if trainer.rdo else None,
            'world_size': world_size,
        }
        
        with open(os.path.join(args.output_dir, 'compression_stats.json'), 'w') as f:
            json.dump(stats, f, indent=2)
        
        logger.info("\n" + "=" * 60)
        logger.info("âœ“ è®­ç»ƒå®Œæˆ!")
        if trainer.rdo:
            logger.info(f"âœ“ æœ€ç»ˆ Î²: {trainer.rdo.beta:.4f}")
        logger.info("=" * 60)
    
    # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
    cleanup_distributed()


if __name__ == "__main__":
    main()