#!/usr/bin/env python3
"""
CIBDè®­ç»ƒè„šæœ¬ - V2 with RDO (Multi-GPU) - FIXED
æ”¯æŒï¼š
1. DataParallel (DP)
2. DistributedDataParallel (DDP) 
3. DeepSpeed

ä¿®å¤ï¼š
1. loss_info è¯»å–é—®é¢˜ï¼ˆModelOutput è‡ªå®šä¹‰å±æ€§ï¼‰
2. æŸå¤±ç´¯ç§¯é€»è¾‘ï¼ˆä½¿ç”¨åŸå§‹æŸå¤±å€¼ï¼‰
3. DDP åŒæ­¥ï¼ˆæ‰€æœ‰åˆ†é¡¹æŸå¤±ï¼‰
4. è°ƒè¯•è¾“å‡ºï¼ˆç¬¬ä¸€ä¸ªbatchï¼‰
"""

import os
import sys
import json
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.utils.data import DataLoader, DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
from transformers import get_cosine_schedule_with_warmup
import argparse
from tqdm import tqdm
import logging
from typing import Optional, Dict, Any
import numpy as np

# ===== PyTorch 2.0+ æ··åˆç²¾åº¦æ”¯æŒ =====
from torch.amp import autocast, GradScaler

# å¯¼å…¥LLaVAç»„ä»¶
from llava.model.builder import load_pretrained_model
from llava.model.language_model.llava_llama import LlavaLlamaForCausalLM
from transformers.modeling_outputs import CausalLMOutputWithPast

# å¯¼å…¥CIBDç»„ä»¶
from cibd.dataset import LLaVADataset, LLaVACollator
from cibd.llava_cibd_model_v2 import LlavaCIBDModelV2
from cibd.create_student_model_v2 import (
    create_compressed_student_config_v2,
    initialize_student_from_teacher_v2,
    estimate_model_params
)

# å¯¼å…¥RDOç»„ä»¶
from cibd.rate_distortion_optimizer import (
    RateDistortionOptimizer,
    AdaptiveRateScheduler
)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    format='%(asctime)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)


# ===== ã€ä¿®å¤1ã€‘å®‰å…¨è¯»å– loss_info =====
def safe_get_loss_info(outputs: Any) -> Dict[str, float]:
    """
    å®‰å…¨åœ°ä» outputs ä¸­æå– loss_info
    å…¼å®¹ ModelOutput è‡ªå®šä¹‰å±æ€§å’Œ dict ä¸¤ç§æ ¼å¼
    """
    # æ–¹æ³•1ï¼šä½œä¸ºå±æ€§è¯»å–ï¼ˆModelOutputï¼‰
    loss_info = getattr(outputs, 'loss_info', None)
    
    # æ–¹æ³•2ï¼šä½œä¸ºå­—å…¸è¯»å–ï¼ˆdictï¼‰
    if loss_info is None and isinstance(outputs, dict):
        loss_info = outputs.get('loss_info', None)
    
    # é»˜è®¤è¿”å›ç©ºå­—å…¸
    if loss_info is None:
        return {}
    
    # è½¬æ¢æ‰€æœ‰å€¼ä¸º float
    def to_float(x):
        if torch.is_tensor(x):
            return float(x.detach().cpu().item())
        return float(x)
    
    return {k: to_float(v) for k, v in loss_info.items()}


def setup_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ["RANK"])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ['LOCAL_RANK'])
    elif 'SLURM_PROCID' in os.environ:
        # SLURM environment
        rank = int(os.environ['SLURM_PROCID'])
        world_size = int(os.environ['SLURM_NTASKS'])
        local_rank = rank % torch.cuda.device_count()
    else:
        rank = 0
        world_size = 1
        local_rank = 0
    
    if world_size > 1:
        torch.cuda.set_device(local_rank)
        dist.init_process_group(
            backend='nccl',
            init_method='env://',
            world_size=world_size,
            rank=rank
        )
        dist.barrier()
    
    return rank, world_size, local_rank


def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()


class CIBDTrainerMultiGPU:
    """
    CIBDè®­ç»ƒå™¨ - å¤šGPUç‰ˆæœ¬
    
    æ”¯æŒï¼š
    1. DataParallel (DP) - ç®€å•å¤šå¡
    2. DistributedDataParallel (DDP) - æ¨èå¤šå¡
    3. DeepSpeed - å¤§æ¨¡å‹ä¼˜åŒ–
    """
    
    def __init__(self, model, train_loader, val_loader, args, device, 
                 rank=0, world_size=1, local_rank=0):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.args = args
        self.device = device
        self.rank = rank
        self.world_size = world_size
        self.local_rank = local_rank
        self.is_main_process = (rank == 0)
        
        # ===== è®¾ç½®æ··åˆç²¾åº¦ =====
        if args.precision == 'bf16' and torch.cuda.is_available() and torch.cuda.is_bf16_supported():
            self.dtype = torch.bfloat16
            self.use_amp = True
            self.scaler = GradScaler('cuda', enabled=False)  # BF16 ä¸éœ€è¦ scaler
            if self.is_main_process:
                logger.info("âœ“ ä½¿ç”¨ BF16 æ··åˆç²¾åº¦ï¼ˆæ¨èç”¨äº H100ï¼‰")
        elif args.precision == 'fp16':
            self.dtype = torch.float16
            self.use_amp = True
            self.scaler = GradScaler('cuda', enabled=True)  # FP16 éœ€è¦ scaler
            if self.is_main_process:
                logger.info("âœ“ ä½¿ç”¨ FP16 æ··åˆç²¾åº¦")
        else:
            self.dtype = torch.float32
            self.use_amp = False
            self.scaler = GradScaler('cuda', enabled=False)
            if self.is_main_process:
                logger.info("âœ“ ä½¿ç”¨ FP32 å…¨ç²¾åº¦")
        # ===========================
        
        try:
            import bitsandbytes as bnb
            self.optimizer = bnb.optim.AdamW8bit(
                model.parameters(),
                lr=args.learning_rate,
                weight_decay=args.weight_decay
            )
            if self.is_main_process:
                logger.info("âœ“ ä½¿ç”¨ 8-bit AdamW")
        except ImportError:
            self.optimizer = torch.optim.AdamW(
                model.parameters(),
                lr=args.learning_rate,
                weight_decay=args.weight_decay
            )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        num_training_steps = len(train_loader) * args.num_epochs
        num_warmup_steps = int(num_training_steps * args.warmup_ratio)
        
        self.scheduler = get_cosine_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=num_warmup_steps,
            num_training_steps=num_training_steps
        )
        
        # RDO ä¼˜åŒ–å™¨ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
        if self.is_main_process:
            self.rdo = RateDistortionOptimizer(
                initial_beta=args.initial_beta,
                target_compression=args.target_compression_rate,
                adaptation_rate=args.beta_adaptation_rate,
                window_size=100
            )
        else:
            self.rdo = None
        
        # è‡ªé€‚åº”å‹ç¼©ç‡è°ƒåº¦å™¨
        if args.use_adaptive_compression:
            self.compression_scheduler = AdaptiveRateScheduler(
                initial_rate=args.initial_compression_rate,
                target_rate=args.target_compression_rate,
                warmup_steps=num_warmup_steps,
                total_steps=num_training_steps
            )
        else:
            self.compression_scheduler = None
        
        # è®­ç»ƒçŠ¶æ€
        self.global_step = 0
        self.current_epoch = 0
        self.best_val_loss = float('inf')
        
        # ===== KD Warmup é…ç½® =====
        self.kd_warmup_steps = args.kd_warmup_steps
        self.initial_kd_weight = args.kd_weight
        if self.is_main_process and self.kd_warmup_steps > 0:
            logger.info(f"âœ“ KD Warmup: {self.kd_warmup_steps} steps (0.0 â†’ {args.kd_weight})")
        # ===========================
        
        # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
        if self.is_main_process:
            os.makedirs(args.output_dir, exist_ok=True)
            self.checkpoint_dir = os.path.join(args.output_dir, 'checkpoints')
            os.makedirs(self.checkpoint_dir, exist_ok=True)
            
            # æ—¥å¿—æ–‡ä»¶
            self.log_file = os.path.join(args.output_dir, 'training_log.txt')
            self.rdo_log_file = os.path.join(args.output_dir, 'rdo_log.txt')
        
        if self.is_main_process:
            logger.info("\n" + "=" * 60)
            logger.info("è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆï¼ˆMulti-GPU with RDO - FIXEDï¼‰")
            logger.info("=" * 60)
            logger.info(f"World Size: {world_size}")
            logger.info(f"Rank: {rank}")
            logger.info(f"Local Rank: {local_rank}")
            logger.info(f"è®­ç»ƒæ ·æœ¬: {len(train_loader.dataset)}")
            logger.info(f"éªŒè¯æ ·æœ¬: {len(val_loader.dataset) if val_loader else 0}")
            logger.info(f"æ€»æ­¥æ•°: {num_training_steps}")
            logger.info(f"é¢„çƒ­æ­¥æ•°: {num_warmup_steps}")
            logger.info(f"å­¦ä¹ ç‡: {args.learning_rate}")
            logger.info(f"æ‰¹æ¬¡å¤§å°: {args.batch_size} (per GPU)")
            logger.info(f"å…¨å±€æ‰¹æ¬¡: {args.batch_size * world_size}")
            logger.info(f"æ¢¯åº¦ç´¯ç§¯: {args.gradient_accumulation_steps}")
            logger.info(f"\nğŸ”§ RDO é…ç½®:")
            logger.info(f"  åˆå§‹ Î²: {args.initial_beta}")
            logger.info(f"  ç›®æ ‡å‹ç¼©ç‡: {args.target_compression_rate}")
            logger.info(f"  Î² è‡ªé€‚åº”ç‡: {args.beta_adaptation_rate}")
            logger.info("=" * 60 + "\n")
            
            # ===== ã€æ–°å¢ã€‘æ£€æŸ¥æ¨¡å‹çŠ¶æ€ =====
            model_to_check = self.model.module if hasattr(self.model, 'module') else self.model
            logger.info(f"\nğŸ” æ¨¡å‹çŠ¶æ€æ£€æŸ¥:")
            logger.info(f"  model.training: {model_to_check.training}")
            if hasattr(model_to_check, 'teacher_model'):
                logger.info(f"  teacher_model exists: {model_to_check.teacher_model is not None}")
                if model_to_check.teacher_model is not None:
                    teacher_frozen = not any(p.requires_grad for p in model_to_check.teacher_model.parameters())
                    logger.info(f"  teacher frozen: {teacher_frozen}")
            logger.info(f"  visual_ib exists: {hasattr(model_to_check, 'visual_ib')}")
            logger.info(f"  multi_layer_distill exists: {hasattr(model_to_check, 'multi_layer_distill') and model_to_check.multi_layer_distill is not None}")
            logger.info("=" * 60 + "\n")
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        if self.is_main_process:
            logger.info("\n" + "=" * 60)
            logger.info("å¼€å§‹è®­ç»ƒï¼ˆMulti-GPU with RDOï¼‰")
            logger.info("=" * 60 + "\n")
        
        for epoch in range(self.current_epoch, self.args.num_epochs):
            self.current_epoch = epoch
            
            # è®¾ç½®åˆ†å¸ƒå¼é‡‡æ ·å™¨çš„epochï¼ˆç¡®ä¿æ¯ä¸ªepochæ•°æ®shuffleä¸åŒï¼‰
            if hasattr(self.train_loader.sampler, 'set_epoch'):
                self.train_loader.sampler.set_epoch(epoch)
            
            if self.is_main_process:
                logger.info(f"\n{'='*60}")
                logger.info(f"Epoch {epoch + 1}/{self.args.num_epochs}")
                logger.info(f"{'='*60}")
            
            # è®­ç»ƒä¸€ä¸ªepoch
            train_metrics = self._train_epoch()
            
            # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
            if self.world_size > 1:
                dist.barrier()
            
            # è®°å½•è®­ç»ƒæŒ‡æ ‡ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
            if self.is_main_process:
                self._log_metrics(epoch, train_metrics, phase='train')
            
            # éªŒè¯
            if self.val_loader is not None:
                val_metrics = self._validate()
                
                if self.is_main_process:
                    self._log_metrics(epoch, val_metrics, phase='val')
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    if val_metrics['total_loss'] < self.best_val_loss:
                        self.best_val_loss = val_metrics['total_loss']
                        self._save_checkpoint('best_model')
                        logger.info(f"âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (val_loss={self.best_val_loss:.4f})")
            
            # å®šæœŸä¿å­˜checkpoint
            if self.is_main_process and (epoch + 1) % self.args.save_steps == 0:
                self._save_checkpoint(f'checkpoint_epoch_{epoch+1}')
            
            # ä¿å­˜Paretoå‰æ²¿
            if self.is_main_process:
                self._save_pareto_frontier(epoch)
            
            # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
            if self.world_size > 1:
                dist.barrier()
        
        if self.is_main_process:
            logger.info("\n" + "=" * 60)
            logger.info("è®­ç»ƒå®Œæˆ!")
            logger.info("=" * 60 + "\n")
    
    def _train_epoch(self):
        """è®­ç»ƒä¸€ä¸ªepoch - ä¿®å¤ç‰ˆ"""
        self.model.train()
        
        # ===== ã€ä¿®å¤2ã€‘ä½¿ç”¨åŸå§‹æŸå¤±ç´¯ç§¯ =====
        total_loss_sum = 0
        num_valid_batches = 0  # è®°å½•å®é™…å¤„ç†çš„æ‰¹æ¬¡æ•°
        
        loss_components_sum = {
            'task_loss': 0,
            'kl_loss': 0,
            'feature_loss': 0,  # â† æ³¨æ„ï¼šæ˜¯ feature_loss ä¸æ˜¯ feat_loss
            'ib_loss': 0,
            'layer_0_loss': 0,
            'layer_1_loss': 0,
            'layer_2_loss': 0,
            # é¢å¤–çš„ç»†èŠ‚é¡¹
            'ib_kl': 0,
            'ib_recon': 0,
            'ib_beta': 0,
            'ib_weight': 0,
            'kd_weight': 0,
            'feat_weight': 0,
        }
        
        # RDO ç»Ÿè®¡ï¼ˆåªåœ¨ä¸»è¿›ç¨‹æ”¶é›†ï¼‰
        rdo_stats = {
            'beta_values': [],
            'compression_rates': [],
            'rate_values': [],
            'distortion_values': []
        } if self.is_main_process else None
        
        self.optimizer.zero_grad()
        
        # åªåœ¨ä¸»è¿›ç¨‹æ˜¾ç¤ºè¿›åº¦æ¡
        if self.is_main_process:
            progress_bar = tqdm(
                self.train_loader,
                desc=f"Epoch {self.current_epoch + 1}",
                dynamic_ncols=True
            )
        else:
            progress_bar = self.train_loader
        
        for step, batch in enumerate(progress_bar):
            try:
                # ===== å¤„ç† images list =====
                images = batch.pop('images', None)
                
                # å…¶ä»–å­—æ®µç§»åˆ°è®¾å¤‡
                batch = {k: v.to(self.device) if torch.is_tensor(v) else v 
                        for k, v in batch.items()}
                
                # å¤„ç† images
                if images is not None:
                    if isinstance(images, list):
                        # è¿‡æ»¤å¹¶å¤„ç† list
                        processed_images = []
                        valid_count = 0
                        
                        for img in images:
                            if img is not None and torch.is_tensor(img):
                                # å…œåº•æ£€æŸ¥ï¼šç¡®ä¿æ˜¯ 3 é€šé“
                                if img.dim() == 3:
                                    if img.shape[0] == 4:
                                        img = img[:3, :, :]
                                        if self.is_main_process and valid_count == 0:
                                            logger.warning(f"Step {step}: å‘ç°4é€šé“å›¾ç‰‡ï¼Œå·²è£å‰ª")
                                    elif img.shape[0] != 3:
                                        if self.is_main_process:
                                            logger.error(f"Step {step}: å¼‚å¸¸é€šé“æ•° {img.shape[0]}")
                                        raise ValueError(f"Invalid channel count: {img.shape[0]}")
                                
                                processed_images.append(img.to(self.device))
                                valid_count += 1
                            else:
                                processed_images.append(None)
                        
                        # ===== ã€DDP ä¿®å¤ã€‘ä¸è¦è·³è¿‡ batchï¼Œå³ä½¿æ²¡æœ‰æœ‰æ•ˆå›¾ç‰‡ =====
                        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆå›¾ç‰‡ï¼Œåˆ›å»ºä¸€ä¸ª dummy tensor ç¡®ä¿æ‰€æœ‰ rank éƒ½æ‰§è¡Œ
                        if valid_count == 0:
                            if self.is_main_process:
                                logger.warning(f"Step {step}: æ²¡æœ‰æœ‰æ•ˆå›¾ç‰‡ï¼Œä½¿ç”¨ dummy tensor")
                            # åˆ›å»º dummy image tensorï¼ˆ3, 224, 224ï¼‰
                            dummy_img = torch.zeros(3, 224, 224, device=self.device)
                            processed_images = [dummy_img] * len(images)
                            valid_count = 1
                        
                        # æ ¹æ®æƒ…å†µè½¬æ¢æ ¼å¼
                        if all(img is not None for img in processed_images):
                            images = torch.stack(processed_images)
                        else:
                            images = processed_images
                        
                    elif torch.is_tensor(images):
                        if images.dim() == 4 and images.shape[1] == 4:
                            images = images[:, :3, :, :]
                            if self.is_main_process:
                                logger.warning(f"Step {step}: å‘ç°4é€šé“æ‰¹æ¬¡ï¼Œå·²è£å‰ª")
                        elif images.dim() == 4 and images.shape[1] != 3:
                            if self.is_main_process:
                                logger.error(f"Step {step}: å¼‚å¸¸é€šé“æ•°æ‰¹æ¬¡ {images.shape[1]}")
                            raise ValueError(f"Invalid channel count: {images.shape[1]}")
                        
                        images = images.to(self.device)
                
                batch['images'] = images
                
                # ===== KD Warmup é€»è¾‘ =====
                if self.kd_warmup_steps > 0 and self.global_step < self.kd_warmup_steps:
                    # çº¿æ€§ warmupï¼šä» 0.0 åˆ° initial_kd_weight
                    current_kd_weight = self.initial_kd_weight * (self.global_step / self.kd_warmup_steps)
                    # åŠ¨æ€è®¾ç½®æ¨¡å‹çš„ KD æƒé‡
                    if hasattr(self.model, 'module'):  # DDP
                        self.model.module.log_kd_weight.data.fill_(torch.log(torch.tensor(current_kd_weight + 1e-8)))
                    else:
                        self.model.log_kd_weight.data.fill_(torch.log(torch.tensor(current_kd_weight + 1e-8)))
                # =============================
                
                # ===== å‰å‘ä¼ æ’­ï¼ˆä½¿ç”¨ autocastï¼‰=====
                if self.use_amp:
                    with autocast('cuda', dtype=self.dtype):
                        outputs = self.model(
                            input_ids=batch['input_ids'],
                            attention_mask=batch['attention_mask'],
                            labels=batch['labels'],
                            images=batch['images'],
                            return_dict=True
                        )
                else:
                    outputs = self.model(
                        input_ids=batch['input_ids'],
                        attention_mask=batch['attention_mask'],
                        labels=batch['labels'],
                        images=batch['images'],
                        return_dict=True
                    )
                
                # ===== ã€ä¿®å¤2ã€‘åœ¨ç¼©æ”¾å‰è®°å½•åŸå§‹æŸå¤± =====
                loss = outputs.loss
                original_loss_value = loss.detach().float().item()  # â† é¿å… AMP ä¸‹çš„ç²¾åº¦é—®é¢˜
                
                # ===== ã€ä¿®å¤1ã€‘ä½¿ç”¨ safe_get_loss_info è¯»å–åˆ†é¡¹æŸå¤± =====
                loss_info = safe_get_loss_info(outputs)
                
                # ===== ã€è°ƒè¯•ã€‘ç¬¬ä¸€ä¸ªbatchæ‰“å°è¯¦ç»†ä¿¡æ¯ =====
                if step == 0 and self.is_main_process:
                    logger.info(f"\n{'='*60}")
                    logger.info(f"DEBUG: ç¬¬ä¸€ä¸ªbatchçš„æŸå¤±è¯¦æƒ…")
                    logger.info(f"{'='*60}")
                    logger.info(f"outputs.loss = {original_loss_value:.4f}")
                    logger.info(f"loss_info å†…å®¹:")
                    for k, v in sorted(loss_info.items()):
                        logger.info(f"  {k}: {v:.4f}")
                    logger.info(f"æ¨¡å‹è®­ç»ƒçŠ¶æ€: {self.model.training}")
                    logger.info(f"{'='*60}\n")
                
                # ===== RDO ä¼˜åŒ–ï¼ˆä¿®å¤ DDP é—®é¢˜ï¼‰=====
                # ã€ä¿®å¤ã€‘å»æ‰æ¡ä»¶åˆ¤æ–­ï¼Œç¡®ä¿æ¯ä¸ª batch éƒ½æ‰§è¡Œï¼Œé¿å… DDP unused parameters é”™è¯¯
                task_loss = loss_info.get('task_loss', 0)
                ib_loss = loss_info.get('ib_loss', 0)
                
                # æ€»æ˜¯æ‰§è¡Œ RDOï¼Œä½¿ç”¨é»˜è®¤å€¼é¿å…é™¤é›¶
                if self.is_main_process:
                    # ä½¿ç”¨ max(loss, 1e-8) ç¡®ä¿éé›¶å€¼
                    rate = loss_info.get('ib_kl', max(ib_loss, 1e-8))
                    distortion = max(task_loss, 1e-8)
                    
                    optimal_beta = self.rdo.step(
                        torch.tensor(distortion).to(self.device),
                        torch.tensor(rate).to(self.device)
                    )
                    
                    rdo_stats['beta_values'].append(optimal_beta)
                    rdo_stats['rate_values'].append(rate)
                    rdo_stats['distortion_values'].append(distortion)
                else:
                    optimal_beta = 1.0
                
                # æ€»æ˜¯åŒæ­¥ betaï¼ˆå¤š GPUï¼‰
                if self.world_size > 1:
                    beta_tensor = torch.tensor(optimal_beta, device=self.device)
                    dist.broadcast(beta_tensor, src=0)
                    optimal_beta = beta_tensor.item()
                
                # æ€»æ˜¯æ›´æ–° visual_ib.log_betaï¼Œç¡®ä¿å‚æ•°è¢«ä½¿ç”¨ï¼ˆé¿å… DDP unused parametersï¼‰
                model_to_update = self.model.module if hasattr(self.model, 'module') else self.model
                if hasattr(model_to_update, 'visual_ib') and hasattr(model_to_update.visual_ib, 'log_beta'):
                    with torch.no_grad():
                        model_to_update.visual_ib.log_beta.data = torch.tensor(
                            np.log(max(optimal_beta, 1e-8)),  # ç¡®ä¿éé›¶
                            dtype=torch.float32,
                            device=self.device
                        )
                
                # ===== æ¢¯åº¦ç´¯ç§¯ =====
                loss = loss / self.args.gradient_accumulation_steps
                
                # ===== åå‘ä¼ æ’­ï¼ˆä½¿ç”¨ GradScalerï¼‰=====
                if self.scaler.is_enabled():
                    self.scaler.scale(loss).backward()
                else:
                    loss.backward()
                
                # ===== ã€ä¿®å¤2ã€‘ç´¯ç§¯åŸå§‹æŸå¤±ï¼ˆæœªç¼©æ”¾çš„ï¼‰=====
                total_loss_sum += original_loss_value
                num_valid_batches += 1
                
                # ===== ã€ä¿®å¤1ã€‘ç´¯ç§¯æ‰€æœ‰åˆ†é¡¹æŸå¤± =====
                for key in loss_components_sum.keys():
                    if key in loss_info:
                        loss_components_sum[key] += loss_info[key]
                
                # ===== å‚æ•°æ›´æ–° =====
                if (step + 1) % self.args.gradient_accumulation_steps == 0:
                    # æ¢¯åº¦è£å‰ª
                    if self.args.max_grad_norm > 0:
                        if self.scaler.is_enabled():
                            self.scaler.unscale_(self.optimizer)
                        torch.nn.utils.clip_grad_norm_(
                            self.model.parameters(),
                            self.args.max_grad_norm
                        )
                    
                    # ä¼˜åŒ–å™¨æ­¥è¿›
                    if self.scaler.is_enabled():
                        self.scaler.step(self.optimizer)
                        self.scaler.update()
                    else:
                        self.optimizer.step()
                    
                    self.scheduler.step()
                    self.optimizer.zero_grad()
                    self.global_step += 1
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    if self.is_main_process:
                        current_lr = self.scheduler.get_last_lr()[0]
                        current_beta = rdo_stats['beta_values'][-1] if rdo_stats and rdo_stats['beta_values'] else 1.0
                        progress_bar.set_postfix({
                            'loss': f'{original_loss_value:.4f}',  # ä½¿ç”¨åŸå§‹æŸå¤±
                            'lr': f'{current_lr:.2e}',
                            'Î²': f'{current_beta:.3f}'
                        })
            
            except Exception as e:
                # ===== ã€DDP ä¿®å¤ã€‘ä¸è¦å•ç‹¬ continueï¼Œè®©å¼‚å¸¸å‘ä¸Šä¼ æ’­ =====
                # å¦‚æœä¸€ä¸ª rank å¼‚å¸¸ï¼Œåº”è¯¥è®©æ‰€æœ‰ rank éƒ½çŸ¥é“å¹¶ä¸€èµ·åœæ­¢
                if self.is_main_process:
                    logger.error(f"Error in training step {step}: {e}")
                    import traceback
                    traceback.print_exc()
                
                # ä¸è¦ continueï¼è®©å¼‚å¸¸ä¼ æ’­ï¼Œæˆ–è€…åšå…¨å±€åŒæ­¥çš„å¤„ç†
                # è¿™é‡Œæˆ‘ä»¬é€‰æ‹©è®©å¼‚å¸¸ä¼ æ’­ï¼Œç»ˆæ­¢è®­ç»ƒ
                raise
        
        # ===== ã€ä¿®å¤3ã€‘DDP åŒæ­¥æ‰€æœ‰GPUçš„æŸå¤± =====
        if self.world_size > 1:
            total_loss_tensor = torch.tensor(total_loss_sum, device=self.device)
            num_batches_tensor = torch.tensor(num_valid_batches, device=self.device)
            
            dist.all_reduce(total_loss_tensor, op=dist.ReduceOp.SUM)
            dist.all_reduce(num_batches_tensor, op=dist.ReduceOp.SUM)
            
            total_loss_sum = total_loss_tensor.item()
            num_valid_batches = num_batches_tensor.item()
            
            # ===== ã€å…³é”®ã€‘å¯¹æ¯ä¸ªæŸå¤±åˆ†é¡¹ä¹Ÿåš all_reduce =====
            for key in loss_components_sum.keys():
                loss_tensor = torch.tensor(loss_components_sum[key], device=self.device)
                dist.all_reduce(loss_tensor, op=dist.ReduceOp.SUM)
                loss_components_sum[key] = loss_tensor.item()
        
        # ===== ã€ä¿®å¤2ã€‘ä½¿ç”¨å®é™…å¤„ç†çš„æ‰¹æ¬¡æ•°è®¡ç®—å¹³å‡ =====
        if num_valid_batches > 0:
            metrics = {
                'total_loss': total_loss_sum / num_valid_batches,
                **{k: v / num_valid_batches for k, v in loss_components_sum.items()}
            }
        else:
            metrics = {
                'total_loss': 0,
                **{k: 0 for k in loss_components_sum}
            }
        
        # æ·»åŠ  RDO ç»Ÿè®¡
        if self.is_main_process and rdo_stats and rdo_stats['beta_values']:
            metrics['avg_beta'] = np.mean(rdo_stats['beta_values'])
            metrics['std_beta'] = np.std(rdo_stats['beta_values'])
            metrics['avg_rate'] = np.mean(rdo_stats['rate_values'])
            metrics['avg_distortion'] = np.mean(rdo_stats['distortion_values'])
            
            if rdo_stats['compression_rates']:
                metrics['avg_compression_rate'] = np.mean(rdo_stats['compression_rates'])
            
            self._log_rdo_stats(rdo_stats)
        
        return metrics
    
    def _validate(self):
        """éªŒè¯ - ä¿®å¤ç‰ˆ"""
        self.model.eval()
        
        # ===== ã€ä¿®å¤2ã€‘ä½¿ç”¨åŸå§‹æŸå¤±ç´¯ç§¯ =====
        total_loss_sum = 0
        num_valid_batches = 0
        
        loss_components_sum = {
            'task_loss': 0,
            'kl_loss': 0,
            'feature_loss': 0,
            'ib_loss': 0,
            'layer_0_loss': 0,
            'layer_1_loss': 0,
            'layer_2_loss': 0,
        }
        
        with torch.no_grad():
            val_iter = tqdm(self.val_loader, desc="Validation", dynamic_ncols=True) if self.is_main_process else self.val_loader
            
            for batch in val_iter:
                # ç§»åŠ¨åˆ°è®¾å¤‡
                batch = {k: v.to(self.device) if torch.is_tensor(v) else v 
                        for k, v in batch.items()}
                
                # ===== å‰å‘ä¼ æ’­ï¼ˆä½¿ç”¨ autocastï¼‰=====
                if self.use_amp:
                    with autocast('cuda', dtype=self.dtype):
                        outputs = self.model(
                            input_ids=batch['input_ids'],
                            attention_mask=batch['attention_mask'],
                            labels=batch['labels'],
                            images=batch.get('images'),
                            image_sizes=batch.get('image_sizes'),
                            return_dict=True
                        )
                else:
                    outputs = self.model(
                        input_ids=batch['input_ids'],
                        attention_mask=batch['attention_mask'],
                        labels=batch['labels'],
                        images=batch.get('images'),
                        image_sizes=batch.get('image_sizes'),
                        return_dict=True
                    )
                
                loss = outputs.loss
                total_loss_sum += loss.detach().float().item()  # â† é¿å… AMP ä¸‹çš„ç²¾åº¦é—®é¢˜
                num_valid_batches += 1
                
                # ===== ã€ä¿®å¤1ã€‘ä½¿ç”¨ safe_get_loss_info =====
                loss_info = safe_get_loss_info(outputs)
                
                for key in loss_components_sum.keys():
                    if key in loss_info:
                        loss_components_sum[key] += loss_info[key]
        
        # ===== ã€ä¿®å¤3ã€‘DDP åŒæ­¥ =====
        if self.world_size > 1:
            total_loss_tensor = torch.tensor(total_loss_sum, device=self.device)
            num_batches_tensor = torch.tensor(num_valid_batches, device=self.device)
            
            dist.all_reduce(total_loss_tensor, op=dist.ReduceOp.SUM)
            dist.all_reduce(num_batches_tensor, op=dist.ReduceOp.SUM)
            
            total_loss_sum = total_loss_tensor.item()
            num_valid_batches = num_batches_tensor.item()
            
            # å¯¹æ¯ä¸ªåˆ†é¡¹ä¹Ÿåš all_reduce
            for key in loss_components_sum.keys():
                loss_tensor = torch.tensor(loss_components_sum[key], device=self.device)
                dist.all_reduce(loss_tensor, op=dist.ReduceOp.SUM)
                loss_components_sum[key] = loss_tensor.item()
        
        # è®¡ç®—å¹³å‡
        if num_valid_batches > 0:
            metrics = {
                'total_loss': total_loss_sum / num_valid_batches,
                **{k: v / num_valid_batches for k, v in loss_components_sum.items()}
            }
        else:
            metrics = {
                'total_loss': 0,
                **{k: 0 for k in loss_components_sum}
            }
        
        return metrics
    
    def _log_metrics(self, epoch, metrics, phase='train'):
        """è®°å½•æŒ‡æ ‡ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰- ä¿®å¤ç‰ˆ"""
        if not self.is_main_process:
            return
        
        log_str = f"\n[{phase.upper()}] Epoch {epoch + 1}\n"
        log_str += f"  Total Loss: {metrics.get('total_loss', 0):.4f}\n"
        log_str += f"  Task Loss: {metrics.get('task_loss', 0):.4f}\n"
        log_str += f"  KL Loss: {metrics.get('kl_loss', 0):.4f}\n"
        log_str += f"  Feature Loss: {metrics.get('feature_loss', 0):.4f}\n"  # â† æ³¨æ„é”®å
        log_str += f"  IB Loss: {metrics.get('ib_loss', 0):.4f}\n"
        
        # å±‚çº§è’¸é¦æŸå¤±
        if any(k in metrics for k in ['layer_0_loss', 'layer_1_loss', 'layer_2_loss']):
            log_str += f"  Layer Losses: "
            log_str += f"{metrics.get('layer_0_loss', 0):.4f} / "
            log_str += f"{metrics.get('layer_1_loss', 0):.4f} / "
            log_str += f"{metrics.get('layer_2_loss', 0):.4f}\n"
        
        # IB ç»†èŠ‚
        if 'ib_kl' in metrics and metrics.get('ib_kl', 0) > 0:
            log_str += f"\n  IB Details:\n"
            log_str += f"    KL: {metrics.get('ib_kl', 0):.4f}\n"
            log_str += f"    Recon: {metrics.get('ib_recon', 0):.4f}\n"
            log_str += f"    Beta: {metrics.get('ib_beta', 1.0):.4f}\n"
        
        # æŸå¤±æƒé‡
        if 'kd_weight' in metrics and metrics.get('kd_weight', 0) > 0:
            log_str += f"\n  Loss Weights:\n"
            log_str += f"    KD: {metrics.get('kd_weight', 1.0):.4f}\n"
            log_str += f"    Feature: {metrics.get('feat_weight', 0.5):.4f}\n"
            log_str += f"    IB: {metrics.get('ib_weight', 0.1):.4f}\n"
        
        # RDO æŒ‡æ ‡
        if 'avg_beta' in metrics:
            log_str += f"\n  ğŸ”§ RDO Stats:\n"
            log_str += f"    Avg Î²: {metrics['avg_beta']:.4f} Â± {metrics.get('std_beta', 0):.4f}\n"
            log_str += f"    Avg Rate: {metrics['avg_rate']:.4f}\n"
            log_str += f"    Avg Distortion: {metrics['avg_distortion']:.4f}\n"
        
        if 'avg_compression_rate' in metrics:
            log_str += f"    Compression Rate: {metrics['avg_compression_rate']:.4f}\n"
        
        logger.info(log_str)
        
        # å†™å…¥æ–‡ä»¶
        with open(self.log_file, 'a') as f:
            f.write(log_str + "\n")
    
    def _log_rdo_stats(self, rdo_stats):
        """è®°å½• RDO è¯¦ç»†ç»Ÿè®¡"""
        if not self.is_main_process or not rdo_stats['beta_values']:
            return
        
        log_str = f"\nStep {self.global_step} RDO Stats:\n"
        log_str += f"  Î²: {np.mean(rdo_stats['beta_values']):.4f}\n"
        log_str += f"  Rate: {np.mean(rdo_stats['rate_values']):.4f}\n"
        log_str += f"  Distortion: {np.mean(rdo_stats['distortion_values']):.4f}\n"
        
        with open(self.rdo_log_file, 'a') as f:
            f.write(log_str)
    
    def _save_pareto_frontier(self, epoch):
        """ä¿å­˜Paretoå‰æ²¿"""
        if not self.is_main_process or self.rdo is None:
            return
        
        rates, distortions = self.rdo.get_pareto_frontier()
        
        if rates:
            pareto_data = {
                'epoch': epoch,
                'rates': rates,
                'distortions': distortions
            }
            
            pareto_file = os.path.join(
                self.args.output_dir, 
                f'pareto_frontier_epoch_{epoch}.json'
            )
            
            with open(pareto_file, 'w') as f:
                json.dump(pareto_data, f, indent=2)
            
            logger.info(f"âœ“ Saved Pareto frontier: {len(rates)} points")
    
    def _save_checkpoint(self, name):
        """ä¿å­˜checkpointï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰"""
        if not self.is_main_process:
            return
        
        checkpoint_path = os.path.join(self.checkpoint_dir, name)
        os.makedirs(checkpoint_path, exist_ok=True)
        
        # è·å–å®é™…çš„æ¨¡å‹ï¼ˆå»é™¤DDP wrapperï¼‰
        model_to_save = self.model.module if hasattr(self.model, 'module') else self.model
        
        # ä¿å­˜æ¨¡å‹
        model_to_save.save_pretrained(checkpoint_path)
        
        # ä¿å­˜è®­ç»ƒçŠ¶æ€
        state = {
            'epoch': self.current_epoch,
            'global_step': self.global_step,
            'best_val_loss': self.best_val_loss,
            'optimizer': self.optimizer.state_dict(),
            'scheduler': self.scheduler.state_dict(),
        }
        
        if self.rdo is not None:
            state['rdo_beta'] = self.rdo.beta
            state['rdo_history'] = {
                'rate_history': list(self.rdo.rate_history),
                'distortion_history': list(self.rdo.distortion_history),
                'beta_history': list(self.rdo.beta_history),
            }
        
        if self.compression_scheduler is not None:
            state['compression_scheduler_step'] = self.compression_scheduler.current_step
        
        torch.save(state, os.path.join(checkpoint_path, 'trainer_state.pt'))
        
        logger.info(f"âœ“ Checkpoint saved: {checkpoint_path}")
    
    def load_checkpoint(self, checkpoint_path):
        """åŠ è½½checkpoint"""
        if self.is_main_process:
            logger.info(f"åŠ è½½checkpoint: {checkpoint_path}")
        
        # åŠ è½½è®­ç»ƒçŠ¶æ€
        state_path = os.path.join(checkpoint_path, 'trainer_state.pt')
        if os.path.exists(state_path):
            state = torch.load(state_path, map_location=self.device)
            self.current_epoch = state['epoch']
            self.global_step = state['global_step']
            self.best_val_loss = state['best_val_loss']
            self.optimizer.load_state_dict(state['optimizer'])
            self.scheduler.load_state_dict(state['scheduler'])
            
            # æ¢å¤ RDO çŠ¶æ€ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
            if self.is_main_process and self.rdo is not None:
                if 'rdo_beta' in state:
                    self.rdo.beta = state['rdo_beta']
                
                if 'rdo_history' in state:
                    from collections import deque
                    self.rdo.rate_history = deque(state['rdo_history']['rate_history'], maxlen=100)
                    self.rdo.distortion_history = deque(state['rdo_history']['distortion_history'], maxlen=100)
                    self.rdo.beta_history = deque(state['rdo_history']['beta_history'], maxlen=100)
            
            if 'compression_scheduler_step' in state and self.compression_scheduler is not None:
                self.compression_scheduler.current_step = state['compression_scheduler_step']
            
            if self.is_main_process:
                logger.info(f"âœ“ ä»epoch {self.current_epoch} æ¢å¤è®­ç»ƒ")
                if self.rdo is not None:
                    logger.info(f"âœ“ RDO Î² æ¢å¤ä¸º: {self.rdo.beta:.4f}")


def parse_args():
    parser = argparse.ArgumentParser(description="CIBD Training Script V2 with RDO (Multi-GPU) - FIXED")
    
    # åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°
    parser.add_argument("--distributed_backend", type=str, default="ddp",
                       choices=["dp", "ddp", "deepspeed"],
                       help="åˆ†å¸ƒå¼è®­ç»ƒåç«¯: dp (DataParallel), ddp (DistributedDataParallel), deepspeed")
    parser.add_argument("--local_rank", type=int, default=-1,
                       help="DDP: local rank (è‡ªåŠ¨è®¾ç½®)")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--teacher_model_path", type=str, required=True)
    parser.add_argument("--compression_ratio", type=float, default=0.5)
    parser.add_argument("--compress_hidden", action="store_true")
    parser.add_argument("--compress_heads", action="store_true")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--train_data_path", type=str, required=True)
    parser.add_argument("--val_data_path", type=str, default=None)
    parser.add_argument("--image_folder", type=str, required=True)
    parser.add_argument("--auto_split", action="store_true")
    parser.add_argument("--val_split", type=float, default=0.1)
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--batch_size", type=int, default=4,
                       help="æ¯ä¸ªGPUçš„batch size")
    parser.add_argument("--num_epochs", type=int, default=3)
    parser.add_argument("--learning_rate", type=float, default=2e-5)
    parser.add_argument("--weight_decay", type=float, default=0.01)
    parser.add_argument("--warmup_ratio", type=float, default=0.03)
    parser.add_argument("--max_grad_norm", type=float, default=1.0)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1)
    
    # æ··åˆç²¾åº¦å‚æ•°
    parser.add_argument("--precision", type=str, default="bf16",
                       choices=["fp32", "fp16", "bf16"],
                       help="è®­ç»ƒç²¾åº¦: fp32 (å…¨ç²¾åº¦), fp16 (FP16æ··åˆç²¾åº¦), bf16 (BF16æ··åˆç²¾åº¦, æ¨èç”¨äºH100)")
    
    # RDO å‚æ•°
    parser.add_argument("--initial_beta", type=float, default=1.0)
    parser.add_argument("--target_compression_rate", type=float, default=0.5)
    parser.add_argument("--beta_adaptation_rate", type=float, default=0.01)
    parser.add_argument("--use_adaptive_compression", action="store_true")
    parser.add_argument("--initial_compression_rate", type=float, default=0.1)
    
    # ===== çŸ¥è¯†è’¸é¦å‚æ•° =====
    parser.add_argument("--kd_temperature", type=float, default=4.0,
                       help="çŸ¥è¯†è’¸é¦æ¸©åº¦ç³»æ•° (2-5, æ¨è 4.0)")
    parser.add_argument("--kd_weight", type=float, default=0.5,
                       help="KD æŸå¤±æƒé‡ (0.1-0.7)")
    parser.add_argument("--feat_weight", type=float, default=0.2,
                       help="ç‰¹å¾è’¸é¦æƒé‡")
    parser.add_argument("--ib_weight", type=float, default=0.1,
                       help="ä¿¡æ¯ç“¶é¢ˆæƒé‡")
    parser.add_argument("--kd_warmup_steps", type=int, default=500,
                       help="KD warmup æ­¥æ•° (0 è¡¨ç¤ºä¸ä½¿ç”¨ warmup)")
    
    # æ•°æ®åŠ è½½
    parser.add_argument("--num_workers", type=int, default=4)
    parser.add_argument("--max_length", type=int, default=2048)
    
    # ä¿å­˜å’Œæ—¥å¿—
    parser.add_argument("--output_dir", type=str, required=True)
    parser.add_argument("--save_steps", type=int, default=1)
    parser.add_argument("--resume", type=str, default=None)
    
    return parser.parse_args()


def main():
    args = parse_args()
    
    # è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒ
    rank, world_size, local_rank = setup_distributed()
    
    # è®¾ç½®è®¾å¤‡
    if world_size > 1:
        device = torch.device(f"cuda:{local_rank}")
        torch.cuda.set_device(device)
    else:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    is_main_process = (rank == 0)
    
    if is_main_process:
        logger.info("\n" + "=" * 60)
        logger.info("CIBD Training V2 with RDO (Multi-GPU) - PyTorch 2.0+")
        logger.info("=" * 60)
        logger.info(f"PyTorch: {torch.__version__}")
        logger.info(f"World Size: {world_size}")
        logger.info(f"Rank: {rank}")
        logger.info(f"Local Rank: {local_rank}")
        logger.info(f"Device: {device}")
        logger.info(f"Backend: {args.distributed_backend}")
        logger.info(f"Batch size per GPU: {args.batch_size}")
        logger.info(f"Global batch size: {args.batch_size * world_size}")
        logger.info(f"\nâš¡ æ··åˆç²¾åº¦é…ç½®:")
        logger.info(f"  Precision: {args.precision.upper()}")
        if args.precision == 'bf16':
            logger.info(f"  BF16 æ”¯æŒ: {torch.cuda.is_bf16_supported()}")
        logger.info("=" * 60 + "\n")
    
    # 1. åŠ è½½æ•™å¸ˆæ¨¡å‹ï¼ˆåªåœ¨ä¸»è¿›ç¨‹æ‰“å°æ—¥å¿—ï¼‰
    if is_main_process:
        logger.info("\n" + "=" * 60)
        logger.info("åŠ è½½æ•™å¸ˆæ¨¡å‹...")
        logger.info("=" * 60)
    
    tokenizer, teacher_model, image_processor, context_len = load_pretrained_model(
        model_path=args.teacher_model_path,
        model_base=None,
        model_name=args.teacher_model_path.split('/')[-1],
        device_map=None
    )
    
    if is_main_process:
        logger.info(f"âœ“ Image processor ç±»å‹: {type(image_processor)}")
        logger.info(f"  Size: {getattr(image_processor, 'size', 'N/A')}")
        logger.info(f"  Crop size: {getattr(image_processor, 'crop_size', 'N/A')}")
    
    # ç¡®ä¿vocabå¤§å°æ­£ç¡®
    if is_main_process:
        logger.info(f"Tokenizer vocab size: {len(tokenizer)}")
    
    if hasattr(teacher_model.config, 'vocab_size'):
        tokenizer_vocab_size = len(tokenizer)
        if teacher_model.config.vocab_size != tokenizer_vocab_size:
            teacher_model.config.vocab_size = tokenizer_vocab_size
            if is_main_process:
                logger.info(f"  âœ“ å·²æ›´æ–° vocab_size ä¸º: {tokenizer_vocab_size}")
    
    teacher_model.to(device)
    teacher_model.eval()
    
    if is_main_process:
        teacher_params = sum(p.numel() for p in teacher_model.parameters())
        logger.info(f"âœ“ æ•™å¸ˆæ¨¡å‹: {teacher_params/1e6:.2f}M å‚æ•°")
    
    # 2. åˆ›å»ºå­¦ç”Ÿæ¨¡å‹
    if is_main_process:
        logger.info("\n" + "=" * 60)
        logger.info("åˆ›å»ºå­¦ç”Ÿæ¨¡å‹...")
        logger.info("=" * 60)
    
    teacher_config = teacher_model.config
    student_config = create_compressed_student_config_v2(
        teacher_config,
        compression_ratio=args.compression_ratio,
        compress_hidden=args.compress_hidden,
        compress_heads=args.compress_heads
    )
    
    # ===== åˆ›å»º CIBD å­¦ç”Ÿæ¨¡å‹ï¼ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼‰=====
    student_model = LlavaCIBDModelV2(
        student_config, 
        teacher_model,
        temperature=args.kd_temperature,  # ä»å‘½ä»¤è¡Œä¼ å…¥
        kd_weight=args.kd_weight,         # ä»å‘½ä»¤è¡Œä¼ å…¥
        feat_weight=args.feat_weight,     # ä»å‘½ä»¤è¡Œä¼ å…¥
        ib_weight=args.ib_weight,         # ä»å‘½ä»¤è¡Œä¼ å…¥
    )
    
    if is_main_process:
        logger.info("âœ“ CIBD è’¸é¦é…ç½®:")
        logger.info(f"  Temperature: {args.kd_temperature} (KD æ¸©åº¦)")
        logger.info(f"  KD Weight: {args.kd_weight}")
        logger.info(f"  Feat Weight: {args.feat_weight}")
        logger.info(f"  IB Weight: {args.ib_weight}")
        if args.kd_warmup_steps > 0:
            logger.info(f"  KD Warmup Steps: {args.kd_warmup_steps}")
    
    # Embedding æ£€æŸ¥
    if hasattr(student_model.model, 'embed_tokens'):
        actual_embed_vocab = student_model.model.embed_tokens.num_embeddings
        if actual_embed_vocab != len(tokenizer):
            student_model.resize_token_embeddings(len(tokenizer))
            if is_main_process:
                logger.info(f"  âœ“ Embedding resized to {len(tokenizer)}")
    
    # åˆå§‹åŒ–æƒé‡
    if is_main_process:
        logger.info("ä»æ•™å¸ˆæ¨¡å‹åˆå§‹åŒ–å­¦ç”Ÿæƒé‡...")
    
    student_model = initialize_student_from_teacher_v2(
        student_model, teacher_model, student_config
    )
    
    # å¤åˆ¶è§†è§‰ç»„ä»¶
    if is_main_process:
        logger.info("å¤åˆ¶è§†è§‰ç»„ä»¶...")
    
    if hasattr(teacher_model, 'model') and hasattr(teacher_model.model, 'vision_tower'):
        student_model.model.vision_tower = teacher_model.model.vision_tower
        for param in student_model.model.vision_tower.parameters():
            param.requires_grad = False
        if is_main_process:
            logger.info("âœ“ Vision towerå·²å¤åˆ¶å¹¶å†»ç»“")
    
    if hasattr(teacher_model, 'model') and hasattr(teacher_model.model, 'mm_projector'):
        student_model.model.mm_projector = teacher_model.model.mm_projector
        if is_main_process:
            logger.info("âœ“ MM projectorå·²å¤åˆ¶")
    
    # å°†æ¨¡å‹ç§»åˆ°è®¾å¤‡å¹¶ä¿æŒ FP32 ç²¾åº¦
    # æ³¨æ„ï¼šæˆ‘ä»¬ä¸å¼ºåˆ¶è½¬æ¢æ¨¡å‹ç²¾åº¦ï¼Œè€Œæ˜¯ä¾èµ– autocast è‡ªåŠ¨æ··åˆç²¾åº¦
    # è¿™æ · LayerNormã€Embedding ç­‰ä¼šè‡ªåŠ¨ä¿æŒ FP32ï¼Œæ›´ç¨³å®š
    student_model.to(device)
    teacher_model.to(device)
    student_model = student_model.float()
    teacher_model = teacher_model.float()
    
    if is_main_process:
        if args.precision == 'bf16':
            logger.info("âœ“ æ¨¡å‹å‚æ•°: FP32, å‰å‘ä¼ æ’­è‡ªåŠ¨ä½¿ç”¨ BF16 (æ¨è)")
        elif args.precision == 'fp16':
            logger.info("âœ“ æ¨¡å‹å‚æ•°: FP32, å‰å‘ä¼ æ’­è‡ªåŠ¨ä½¿ç”¨ FP16 (æ¨è)")
        else:
            logger.info("âœ“ æ¨¡å‹å‚æ•°: FP32")
    
    # ===== å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰=====
    if hasattr(student_model.model, 'gradient_checkpointing_enable'):
        # ä½¿ç”¨æ¨èçš„ use_reentrant=Falseï¼ˆPyTorch 2.0+ æ–°æ–¹å¼ï¼‰
        try:
            student_model.model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={"use_reentrant": False})
            if is_main_process:
                logger.info("âœ“ å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆuse_reentrant=Falseï¼‰")
        except TypeError:
            # æ—§ç‰ˆæœ¬ä¸æ”¯æŒå‚æ•°ï¼Œä½¿ç”¨é»˜è®¤æ–¹å¼
            student_model.model.gradient_checkpointing_enable()
            if is_main_process:
                logger.info("âœ“ å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆé»˜è®¤æ¨¡å¼ï¼‰")
    
    # åŒ…è£…æ¨¡å‹ï¼ˆå¤šGPUï¼‰
    if world_size > 1:
        if args.distributed_backend == "ddp":
            # ===== ä¿®å¤ DDP é—®é¢˜ =====
            # 1. ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½éœ€è¦æ¢¯åº¦æˆ–è€…è¢«å†»ç»“
            # 2. ä½¿ç”¨é™æ€å›¾æ¨¡å¼ï¼ˆPyTorch 2.0+ï¼‰
            student_model = DDP(
                student_model,
                device_ids=[local_rank],
                output_device=local_rank,
                find_unused_parameters=False,  # â† æ”¹ä¸º Falseï¼Œä½¿ç”¨é™æ€å›¾
                broadcast_buffers=True,
                gradient_as_bucket_view=True   # ä¼˜åŒ–æ˜¾å­˜
            )
            
            # ===== å¯ç”¨é™æ€å›¾æ¨¡å¼ï¼ˆPyTorch 2.0+ï¼‰=====
            # è¿™å‘Šè¯‰ DDPï¼šæ¨¡å‹çš„è®¡ç®—å›¾æ˜¯å›ºå®šçš„
            if hasattr(student_model, '_set_static_graph'):
                student_model._set_static_graph()
                if is_main_process:
                    logger.info("âœ“ ä½¿ç”¨ DistributedDataParallel (é™æ€å›¾æ¨¡å¼)")
            else:
                if is_main_process:
                    logger.info("âœ“ ä½¿ç”¨ DistributedDataParallel")
        elif args.distributed_backend == "dp":
            student_model = nn.DataParallel(student_model)
            if is_main_process:
                logger.info("âœ“ ä½¿ç”¨ DataParallel")
    else:
        if is_main_process:
            logger.info("âœ“ ä½¿ç”¨å•GPUè®­ç»ƒ")
    
    if is_main_process:
        student_params = sum(p.numel() for p in student_model.parameters())
        logger.info(f"\nå‹ç¼©åˆ†æ:")
        logger.info(f"  æ•™å¸ˆ: {teacher_params/1e9:.2f}B")
        logger.info(f"  å­¦ç”Ÿ: {student_params/1e9:.2f}B")
        logger.info(f"  å‹ç¼©ç‡: {(1 - student_params/teacher_params):.2%}")
    
    # 3. å‡†å¤‡æ•°æ®
    if is_main_process:
        logger.info("\n" + "=" * 60)
        logger.info("å‡†å¤‡æ•°æ®é›†...")
        logger.info("=" * 60)
    
    # å¤„ç†æ•°æ®åˆ†å‰²
    if args.val_data_path:
        train_json = args.train_data_path
        val_json = args.val_data_path
    elif args.auto_split:
        with open(args.train_data_path, 'r') as f:
            all_data = json.load(f)
        
        split_idx = int(len(all_data) * (1 - args.val_split))
        train_data = all_data[:split_idx]
        val_data = all_data[split_idx:]
        
        if is_main_process:
            os.makedirs(args.output_dir, exist_ok=True)
            train_json = os.path.join(args.output_dir, 'train_split.json')
            val_json = os.path.join(args.output_dir, 'val_split.json')
            
            with open(train_json, 'w') as f:
                json.dump(train_data, f)
            with open(val_json, 'w') as f:
                json.dump(val_data, f)
            
            logger.info(f"åˆ†å‰²: {len(train_data)} è®­ç»ƒ, {len(val_data)} éªŒè¯")
        
        # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
        if world_size > 1:
            dist.barrier()
            # éä¸»è¿›ç¨‹ä¹Ÿéœ€è¦çŸ¥é“æ–‡ä»¶è·¯å¾„
            if not is_main_process:
                train_json = os.path.join(args.output_dir, 'train_split.json')
                val_json = os.path.join(args.output_dir, 'val_split.json')
    else:
        train_json = args.train_data_path
        val_json = None
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = LLaVADataset(
        train_json,
        args.image_folder,
        tokenizer,
        image_processor,
        max_length=args.max_length,
        is_training=True
    )
    
    val_dataset = None
    if val_json:
        val_dataset = LLaVADataset(
            val_json,
            args.image_folder,
            tokenizer,
            image_processor,
            max_length=args.max_length,
            is_training=False
        )
    
    if is_main_process:
        logger.info(f"âœ“ è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
        if val_dataset:
            logger.info(f"âœ“ éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
    
    # æ•°æ®åŠ è½½å™¨ï¼ˆä½¿ç”¨DistributedSamplerï¼‰
    collator = LLaVACollator(tokenizer, image_processor)
    
    if world_size > 1 and args.distributed_backend == "ddp":
        train_sampler = DistributedSampler(
            train_dataset,
            num_replicas=world_size,
            rank=rank,
            shuffle=True
        )
        shuffle = False
    else:
        train_sampler = None
        shuffle = True
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=shuffle,
        sampler=train_sampler,
        collate_fn=collator,
        num_workers=args.num_workers,
        pin_memory=True
    )
    
    val_loader = None
    if val_dataset:
        if world_size > 1 and args.distributed_backend == "ddp":
            val_sampler = DistributedSampler(
                val_dataset,
                num_replicas=world_size,
                rank=rank,
                shuffle=False
            )
        else:
            val_sampler = None
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=args.batch_size,
            shuffle=False,
            sampler=val_sampler,
            collate_fn=collator,
            num_workers=args.num_workers,
            pin_memory=True
        )
    
    # 4. åˆ›å»ºè®­ç»ƒå™¨
    trainer = CIBDTrainerMultiGPU(
        student_model, train_loader, val_loader, args, device,
        rank=rank, world_size=world_size, local_rank=local_rank
    )
    
    # æ¢å¤è®­ç»ƒ
    if args.resume:
        trainer.load_checkpoint(args.resume)
    
    # 5. å¼€å§‹è®­ç»ƒ
    trainer.train()
    
    # 6. ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
    if is_main_process:
        logger.info("\n" + "=" * 60)
        logger.info("ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
        logger.info("=" * 60)
        
        final_path = os.path.join(args.output_dir, 'final_model')
        
        # è·å–å®é™…çš„æ¨¡å‹
        model_to_save = student_model.module if hasattr(student_model, 'module') else student_model
        model_to_save.save_pretrained(final_path)
        tokenizer.save_pretrained(final_path)
        
        logger.info(f"âœ“ æœ€ç»ˆæ¨¡å‹: {final_path}")
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        student_params = sum(p.numel() for p in model_to_save.parameters())
        stats = {
            'teacher_params': teacher_params,
            'student_params': student_params,
            'compression_ratio': 1 - (student_params / teacher_params),
            'config': student_config.to_dict(),
            'final_beta': trainer.rdo.beta if trainer.rdo else None,
            'world_size': world_size,
        }
        
        with open(os.path.join(args.output_dir, 'compression_stats.json'), 'w') as f:
            json.dump(stats, f, indent=2)
        
        logger.info("\n" + "=" * 60)
        logger.info("âœ“ è®­ç»ƒå®Œæˆ!")
        if trainer.rdo:
            logger.info(f"âœ“ æœ€ç»ˆ Î²: {trainer.rdo.beta:.4f}")
        logger.info("=" * 60)
    
    # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
    cleanup_distributed()


if __name__ == "__main__":
    main()